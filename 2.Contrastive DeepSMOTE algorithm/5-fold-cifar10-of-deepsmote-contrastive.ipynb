{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ac536f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T16:19:20.486668Z",
     "iopub.status.busy": "2023-04-11T16:19:20.486242Z",
     "iopub.status.idle": "2023-04-11T16:19:31.287395Z",
     "shell.execute_reply": "2023-04-11T16:19:31.286261Z"
    },
    "papermill": {
     "duration": 10.80793,
     "end_time": "2023-04-11T16:19:31.290001",
     "exception": false,
     "start_time": "2023-04-11T16:19:20.482071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-metric-learning\r\n",
      "  Downloading pytorch_metric_learning-2.1.0-py3-none-any.whl (110 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pytorch-metric-learning) (4.64.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch-metric-learning) (1.21.6)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-metric-learning) (1.11.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from pytorch-metric-learning) (1.0.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->pytorch-metric-learning) (4.1.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pytorch-metric-learning) (1.7.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pytorch-metric-learning) (1.0.1)\r\n",
      "Installing collected packages: pytorch-metric-learning\r\n",
      "Successfully installed pytorch-metric-learning-2.1.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-metric-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3518bc25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T16:19:31.296863Z",
     "iopub.status.busy": "2023-04-11T16:19:31.296553Z",
     "iopub.status.idle": "2023-04-11T16:19:45.987375Z",
     "shell.execute_reply": "2023-04-11T16:19:45.986126Z"
    },
    "papermill": {
     "duration": 14.69699,
     "end_time": "2023-04-11T16:19:45.989910",
     "exception": false,
     "start_time": "2023-04-11T16:19:31.292920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu\r\n",
      "  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\r\n",
      "Successfully installed faiss-gpu-1.7.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233865bd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-11T16:19:46.004168Z",
     "iopub.status.busy": "2023-04-11T16:19:46.003827Z",
     "iopub.status.idle": "2023-04-11T16:19:46.031378Z",
     "shell.execute_reply": "2023-04-11T16:19:46.029979Z"
    },
    "papermill": {
     "duration": 0.037269,
     "end_time": "2023-04-11T16:19:46.033386",
     "exception": false,
     "start_time": "2023-04-11T16:19:45.996117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cifar10-imb-5cv/imb_train_data/1_trn_img_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_data/4_trn_img_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_data/2_trn_img_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_data/0_trn_img_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_data/3_trn_img_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_label/3_trn_lab_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_label/0_trn_lab_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_label/4_trn_lab_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_label/2_trn_lab_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_label/1_trn_lab_imb\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402d1342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T16:19:46.046218Z",
     "iopub.status.busy": "2023-04-11T16:19:46.045933Z",
     "iopub.status.idle": "2023-04-11T16:19:48.660386Z",
     "shell.execute_reply": "2023-04-11T16:19:48.658515Z"
    },
    "papermill": {
     "duration": 2.623183,
     "end_time": "2023-04-11T16:19:48.662609",
     "exception": false,
     "start_time": "2023-04-11T16:19:46.039426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from pytorch_metric_learning import miners, losses, distances, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8921c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T16:19:48.675972Z",
     "iopub.status.busy": "2023-04-11T16:19:48.675500Z",
     "iopub.status.idle": "2023-04-11T18:26:47.560966Z",
     "shell.execute_reply": "2023-04-11T18:26:47.558937Z"
    },
    "papermill": {
     "duration": 7618.894651,
     "end_time": "2023-04-11T18:26:47.563219",
     "exception": false,
     "start_time": "2023-04-11T16:19:48.668568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/kaggle/input/cifar10-imb-5cv/imb_train_data/0_trn_img_imb', '/kaggle/input/cifar10-imb-5cv/imb_train_data/1_trn_img_imb', '/kaggle/input/cifar10-imb-5cv/imb_train_data/2_trn_img_imb', '/kaggle/input/cifar10-imb-5cv/imb_train_data/3_trn_img_imb', '/kaggle/input/cifar10-imb-5cv/imb_train_data/4_trn_img_imb']\n",
      "['/kaggle/input/cifar10-imb-5cv/imb_train_label/0_trn_lab_imb', '/kaggle/input/cifar10-imb-5cv/imb_train_label/1_trn_lab_imb', '/kaggle/input/cifar10-imb-5cv/imb_train_label/2_trn_lab_imb', '/kaggle/input/cifar10-imb-5cv/imb_train_label/3_trn_lab_imb', '/kaggle/input/cifar10-imb-5cv/imb_train_label/4_trn_lab_imb']\n",
      "\n",
      "0\n",
      "cuda\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_data/0_trn_img_imb\n",
      "/kaggle/input/cifar10-imb-5cv/imb_train_label/0_trn_lab_imb\n",
      "train imgs before reshape  (10280, 3072)\n",
      "train labels  (10280,)\n",
      "Counter({0.0: 4500, 1.0: 2000, 2.0: 1000, 3.0: 800, 4.0: 600, 5.0: 500, 6.0: 400, 7.0: 250, 8.0: 150, 9.0: 80})\n",
      "train imgs after reshape  (10280, 3, 32, 32)\n",
      "Epoch: 0 \tTrain Loss: 57.887550 \tmse loss: 18.079029 \tmse2 loss: 18.790315 \tcontrastive loss: 21.018205\n",
      "Saving..\n",
      "Epoch: 1 \tTrain Loss: 43.064694 \tmse loss: 10.584728 \tmse2 loss: 11.828653 \tcontrastive loss: 20.651314\n",
      "Saving..\n",
      "Epoch: 2 \tTrain Loss: 39.527386 \tmse loss: 8.833965 \tmse2 loss: 10.135889 \tcontrastive loss: 20.557533\n",
      "Saving..\n",
      "Epoch: 3 \tTrain Loss: 37.614111 \tmse loss: 8.016851 \tmse2 loss: 9.188014 \tcontrastive loss: 20.409247\n",
      "Saving..\n",
      "Epoch: 4 \tTrain Loss: 36.031781 \tmse loss: 7.445633 \tmse2 loss: 8.371447 \tcontrastive loss: 20.214700\n",
      "Saving..\n",
      "Epoch: 5 \tTrain Loss: 35.204950 \tmse loss: 6.933911 \tmse2 loss: 7.898711 \tcontrastive loss: 20.372327\n",
      "Saving..\n",
      "Epoch: 6 \tTrain Loss: 33.718440 \tmse loss: 6.560355 \tmse2 loss: 7.231193 \tcontrastive loss: 19.926891\n",
      "Saving..\n",
      "Epoch: 7 \tTrain Loss: 32.964731 \tmse loss: 6.337053 \tmse2 loss: 6.723217 \tcontrastive loss: 19.904460\n",
      "Saving..\n",
      "Epoch: 8 \tTrain Loss: 32.257620 \tmse loss: 6.101207 \tmse2 loss: 6.557194 \tcontrastive loss: 19.599219\n",
      "Saving..\n",
      "Epoch: 9 \tTrain Loss: 31.342407 \tmse loss: 5.874906 \tmse2 loss: 5.964542 \tcontrastive loss: 19.502960\n",
      "Saving..\n",
      "Epoch: 10 \tTrain Loss: 31.359411 \tmse loss: 5.717606 \tmse2 loss: 5.875238 \tcontrastive loss: 19.766566\n",
      "Epoch: 11 \tTrain Loss: 30.695124 \tmse loss: 5.603076 \tmse2 loss: 5.652372 \tcontrastive loss: 19.439676\n",
      "Saving..\n",
      "Epoch: 12 \tTrain Loss: 30.363109 \tmse loss: 5.588360 \tmse2 loss: 5.498493 \tcontrastive loss: 19.276256\n",
      "Saving..\n",
      "Epoch: 13 \tTrain Loss: 29.698514 \tmse loss: 5.397490 \tmse2 loss: 5.318006 \tcontrastive loss: 18.983018\n",
      "Saving..\n",
      "Epoch: 14 \tTrain Loss: 29.527607 \tmse loss: 5.395499 \tmse2 loss: 5.348648 \tcontrastive loss: 18.783460\n",
      "Saving..\n",
      "Epoch: 15 \tTrain Loss: 29.194354 \tmse loss: 5.216760 \tmse2 loss: 5.017222 \tcontrastive loss: 18.960373\n",
      "Saving..\n",
      "Epoch: 16 \tTrain Loss: 29.361002 \tmse loss: 5.201892 \tmse2 loss: 4.977529 \tcontrastive loss: 19.181582\n",
      "Epoch: 17 \tTrain Loss: 28.757974 \tmse loss: 5.161757 \tmse2 loss: 4.895848 \tcontrastive loss: 18.700369\n",
      "Saving..\n",
      "Epoch: 18 \tTrain Loss: 28.366438 \tmse loss: 5.043985 \tmse2 loss: 4.786950 \tcontrastive loss: 18.535503\n",
      "Saving..\n",
      "Epoch: 19 \tTrain Loss: 28.320190 \tmse loss: 4.942154 \tmse2 loss: 4.634355 \tcontrastive loss: 18.743681\n",
      "Saving..\n",
      "Epoch: 20 \tTrain Loss: 28.214324 \tmse loss: 4.901624 \tmse2 loss: 4.484126 \tcontrastive loss: 18.828574\n",
      "Saving..\n",
      "Epoch: 21 \tTrain Loss: 27.787769 \tmse loss: 4.736108 \tmse2 loss: 4.327079 \tcontrastive loss: 18.724582\n",
      "Saving..\n",
      "Epoch: 22 \tTrain Loss: 27.850846 \tmse loss: 4.839024 \tmse2 loss: 4.518558 \tcontrastive loss: 18.493264\n",
      "Epoch: 23 \tTrain Loss: 27.935778 \tmse loss: 4.769101 \tmse2 loss: 4.441276 \tcontrastive loss: 18.725401\n",
      "Epoch: 24 \tTrain Loss: 27.598808 \tmse loss: 4.728584 \tmse2 loss: 4.480339 \tcontrastive loss: 18.389885\n",
      "Saving..\n",
      "Epoch: 25 \tTrain Loss: 27.197417 \tmse loss: 4.757638 \tmse2 loss: 4.396784 \tcontrastive loss: 18.042995\n",
      "Saving..\n",
      "Epoch: 26 \tTrain Loss: 27.033378 \tmse loss: 4.621737 \tmse2 loss: 4.261510 \tcontrastive loss: 18.150131\n",
      "Saving..\n",
      "Epoch: 27 \tTrain Loss: 26.689778 \tmse loss: 4.723625 \tmse2 loss: 4.465639 \tcontrastive loss: 17.500514\n",
      "Saving..\n",
      "Epoch: 28 \tTrain Loss: 26.495416 \tmse loss: 4.616605 \tmse2 loss: 4.152675 \tcontrastive loss: 17.726136\n",
      "Saving..\n",
      "Epoch: 29 \tTrain Loss: 26.432326 \tmse loss: 4.642011 \tmse2 loss: 4.250827 \tcontrastive loss: 17.539489\n",
      "Saving..\n",
      "Epoch: 30 \tTrain Loss: 26.039771 \tmse loss: 4.532347 \tmse2 loss: 4.220305 \tcontrastive loss: 17.287119\n",
      "Saving..\n",
      "Epoch: 31 \tTrain Loss: 25.960318 \tmse loss: 4.522883 \tmse2 loss: 4.049633 \tcontrastive loss: 17.387802\n",
      "Saving..\n",
      "Epoch: 32 \tTrain Loss: 25.601812 \tmse loss: 4.524317 \tmse2 loss: 3.957589 \tcontrastive loss: 17.119907\n",
      "Saving..\n",
      "Epoch: 33 \tTrain Loss: 25.531631 \tmse loss: 4.588455 \tmse2 loss: 3.943158 \tcontrastive loss: 17.000018\n",
      "Saving..\n",
      "Epoch: 34 \tTrain Loss: 24.833521 \tmse loss: 4.477214 \tmse2 loss: 3.972131 \tcontrastive loss: 16.384175\n",
      "Saving..\n",
      "Epoch: 35 \tTrain Loss: 25.020127 \tmse loss: 4.422839 \tmse2 loss: 3.893313 \tcontrastive loss: 16.703975\n",
      "Epoch: 36 \tTrain Loss: 24.237412 \tmse loss: 4.416376 \tmse2 loss: 3.862180 \tcontrastive loss: 15.958856\n",
      "Saving..\n",
      "Epoch: 37 \tTrain Loss: 24.535930 \tmse loss: 4.382972 \tmse2 loss: 3.781028 \tcontrastive loss: 16.371930\n",
      "Epoch: 38 \tTrain Loss: 24.697029 \tmse loss: 4.378209 \tmse2 loss: 3.940629 \tcontrastive loss: 16.378192\n",
      "Epoch: 39 \tTrain Loss: 23.968775 \tmse loss: 4.375548 \tmse2 loss: 3.883111 \tcontrastive loss: 15.710116\n",
      "Saving..\n",
      "Epoch: 40 \tTrain Loss: 24.351946 \tmse loss: 4.445826 \tmse2 loss: 3.779738 \tcontrastive loss: 16.126382\n",
      "Epoch: 41 \tTrain Loss: 23.840724 \tmse loss: 4.302678 \tmse2 loss: 3.691713 \tcontrastive loss: 15.846333\n",
      "Saving..\n",
      "Epoch: 42 \tTrain Loss: 23.412290 \tmse loss: 4.264426 \tmse2 loss: 3.799927 \tcontrastive loss: 15.347938\n",
      "Saving..\n",
      "Epoch: 43 \tTrain Loss: 22.897807 \tmse loss: 4.252635 \tmse2 loss: 3.678629 \tcontrastive loss: 14.966543\n",
      "Saving..\n",
      "Epoch: 44 \tTrain Loss: 22.955721 \tmse loss: 4.272851 \tmse2 loss: 3.720994 \tcontrastive loss: 14.961877\n",
      "Epoch: 45 \tTrain Loss: 22.603735 \tmse loss: 4.312351 \tmse2 loss: 3.648120 \tcontrastive loss: 14.643264\n",
      "Saving..\n",
      "Epoch: 46 \tTrain Loss: 22.478518 \tmse loss: 4.241681 \tmse2 loss: 3.767308 \tcontrastive loss: 14.469529\n",
      "Saving..\n",
      "Epoch: 47 \tTrain Loss: 22.580513 \tmse loss: 4.258646 \tmse2 loss: 3.702164 \tcontrastive loss: 14.619704\n",
      "Epoch: 48 \tTrain Loss: 22.390027 \tmse loss: 4.225794 \tmse2 loss: 3.637193 \tcontrastive loss: 14.527039\n",
      "Saving..\n",
      "Epoch: 49 \tTrain Loss: 21.669475 \tmse loss: 4.173255 \tmse2 loss: 3.656223 \tcontrastive loss: 13.839996\n",
      "Saving..\n",
      "Epoch: 50 \tTrain Loss: 21.230270 \tmse loss: 4.112803 \tmse2 loss: 3.485746 \tcontrastive loss: 13.631721\n",
      "Saving..\n",
      "Epoch: 51 \tTrain Loss: 21.219224 \tmse loss: 4.141464 \tmse2 loss: 3.824883 \tcontrastive loss: 13.252876\n",
      "Saving..\n",
      "Epoch: 52 \tTrain Loss: 20.918484 \tmse loss: 4.125480 \tmse2 loss: 3.442147 \tcontrastive loss: 13.350857\n",
      "Saving..\n",
      "Epoch: 53 \tTrain Loss: 20.532216 \tmse loss: 4.225007 \tmse2 loss: 3.497636 \tcontrastive loss: 12.809573\n",
      "Saving..\n",
      "Epoch: 54 \tTrain Loss: 20.191908 \tmse loss: 4.107558 \tmse2 loss: 3.650219 \tcontrastive loss: 12.434131\n",
      "Saving..\n",
      "Epoch: 55 \tTrain Loss: 20.026572 \tmse loss: 4.081777 \tmse2 loss: 3.383108 \tcontrastive loss: 12.561687\n",
      "Saving..\n",
      "Epoch: 56 \tTrain Loss: 20.418183 \tmse loss: 4.116752 \tmse2 loss: 3.692508 \tcontrastive loss: 12.608924\n",
      "Epoch: 57 \tTrain Loss: 19.060861 \tmse loss: 4.042256 \tmse2 loss: 3.371145 \tcontrastive loss: 11.647460\n",
      "Saving..\n",
      "Epoch: 58 \tTrain Loss: 19.830779 \tmse loss: 4.109003 \tmse2 loss: 3.478736 \tcontrastive loss: 12.243040\n",
      "Epoch: 59 \tTrain Loss: 19.923777 \tmse loss: 4.156362 \tmse2 loss: 3.427994 \tcontrastive loss: 12.339421\n",
      "Epoch: 60 \tTrain Loss: 18.829712 \tmse loss: 3.991479 \tmse2 loss: 3.401181 \tcontrastive loss: 11.437052\n",
      "Saving..\n",
      "Epoch: 61 \tTrain Loss: 18.669143 \tmse loss: 3.913711 \tmse2 loss: 3.329333 \tcontrastive loss: 11.426099\n",
      "Saving..\n",
      "Epoch: 62 \tTrain Loss: 19.221527 \tmse loss: 3.987376 \tmse2 loss: 3.283355 \tcontrastive loss: 11.950796\n",
      "Epoch: 63 \tTrain Loss: 19.027624 \tmse loss: 3.936426 \tmse2 loss: 3.241775 \tcontrastive loss: 11.849423\n",
      "Epoch: 64 \tTrain Loss: 18.421692 \tmse loss: 3.895104 \tmse2 loss: 3.236568 \tcontrastive loss: 11.290020\n",
      "Saving..\n",
      "Epoch: 65 \tTrain Loss: 17.524510 \tmse loss: 3.909920 \tmse2 loss: 3.221363 \tcontrastive loss: 10.393226\n",
      "Saving..\n",
      "Epoch: 66 \tTrain Loss: 17.354836 \tmse loss: 3.877856 \tmse2 loss: 3.025769 \tcontrastive loss: 10.451210\n",
      "Saving..\n",
      "Epoch: 67 \tTrain Loss: 17.927909 \tmse loss: 3.903507 \tmse2 loss: 3.434893 \tcontrastive loss: 10.589509\n",
      "Epoch: 68 \tTrain Loss: 17.686245 \tmse loss: 3.962023 \tmse2 loss: 3.470133 \tcontrastive loss: 10.254089\n",
      "Epoch: 69 \tTrain Loss: 18.042904 \tmse loss: 3.894188 \tmse2 loss: 3.154457 \tcontrastive loss: 10.994260\n",
      "Epoch: 70 \tTrain Loss: 17.443861 \tmse loss: 4.019638 \tmse2 loss: 3.321457 \tcontrastive loss: 10.102766\n",
      "Epoch: 71 \tTrain Loss: 18.464941 \tmse loss: 3.937409 \tmse2 loss: 3.219870 \tcontrastive loss: 11.307662\n",
      "Epoch: 72 \tTrain Loss: 17.297741 \tmse loss: 3.946223 \tmse2 loss: 3.121944 \tcontrastive loss: 10.229575\n",
      "Saving..\n",
      "Epoch: 73 \tTrain Loss: 17.687734 \tmse loss: 3.915208 \tmse2 loss: 3.279819 \tcontrastive loss: 10.492707\n",
      "Epoch: 74 \tTrain Loss: 16.629478 \tmse loss: 3.848802 \tmse2 loss: 3.141449 \tcontrastive loss: 9.639227\n",
      "Saving..\n",
      "Epoch: 75 \tTrain Loss: 16.703087 \tmse loss: 3.774660 \tmse2 loss: 3.118916 \tcontrastive loss: 9.809511\n",
      "Epoch: 76 \tTrain Loss: 16.916883 \tmse loss: 3.780179 \tmse2 loss: 3.010644 \tcontrastive loss: 10.126059\n",
      "Epoch: 77 \tTrain Loss: 17.417983 \tmse loss: 3.813099 \tmse2 loss: 3.070380 \tcontrastive loss: 10.534504\n",
      "Epoch: 78 \tTrain Loss: 16.264742 \tmse loss: 3.827279 \tmse2 loss: 3.116220 \tcontrastive loss: 9.321244\n",
      "Saving..\n",
      "Epoch: 79 \tTrain Loss: 16.292106 \tmse loss: 3.750890 \tmse2 loss: 2.950240 \tcontrastive loss: 9.590976\n",
      "Epoch: 80 \tTrain Loss: 17.158938 \tmse loss: 3.794576 \tmse2 loss: 3.148371 \tcontrastive loss: 10.215991\n",
      "Epoch: 81 \tTrain Loss: 16.049128 \tmse loss: 3.704126 \tmse2 loss: 2.857963 \tcontrastive loss: 9.487038\n",
      "Saving..\n",
      "Epoch: 82 \tTrain Loss: 15.926649 \tmse loss: 3.682856 \tmse2 loss: 3.027815 \tcontrastive loss: 9.215978\n",
      "Saving..\n",
      "Epoch: 83 \tTrain Loss: 16.387891 \tmse loss: 3.697251 \tmse2 loss: 3.094383 \tcontrastive loss: 9.596257\n",
      "Epoch: 84 \tTrain Loss: 17.852657 \tmse loss: 3.897442 \tmse2 loss: 3.137727 \tcontrastive loss: 10.817488\n",
      "Epoch: 85 \tTrain Loss: 16.331795 \tmse loss: 3.899690 \tmse2 loss: 2.953835 \tcontrastive loss: 9.478271\n",
      "Epoch: 86 \tTrain Loss: 16.666574 \tmse loss: 3.799098 \tmse2 loss: 3.056032 \tcontrastive loss: 9.811445\n",
      "Epoch: 87 \tTrain Loss: 15.596301 \tmse loss: 3.650144 \tmse2 loss: 2.932032 \tcontrastive loss: 9.014125\n",
      "Saving..\n",
      "Epoch: 88 \tTrain Loss: 17.034104 \tmse loss: 3.728926 \tmse2 loss: 3.223420 \tcontrastive loss: 10.081757\n",
      "Epoch: 89 \tTrain Loss: 15.699635 \tmse loss: 3.706174 \tmse2 loss: 3.036806 \tcontrastive loss: 8.956655\n",
      "Epoch: 90 \tTrain Loss: 16.938705 \tmse loss: 3.715592 \tmse2 loss: 3.092387 \tcontrastive loss: 10.130725\n",
      "Epoch: 91 \tTrain Loss: 15.674700 \tmse loss: 3.660870 \tmse2 loss: 2.925609 \tcontrastive loss: 9.088221\n",
      "Epoch: 92 \tTrain Loss: 15.553887 \tmse loss: 3.663801 \tmse2 loss: 2.962226 \tcontrastive loss: 8.927859\n",
      "Saving..\n",
      "Epoch: 93 \tTrain Loss: 15.157016 \tmse loss: 3.524969 \tmse2 loss: 2.625447 \tcontrastive loss: 9.006600\n",
      "Saving..\n",
      "Epoch: 94 \tTrain Loss: 15.461343 \tmse loss: 3.533617 \tmse2 loss: 2.656349 \tcontrastive loss: 9.271376\n",
      "Epoch: 95 \tTrain Loss: 15.205755 \tmse loss: 3.568859 \tmse2 loss: 2.889201 \tcontrastive loss: 8.747695\n",
      "Epoch: 96 \tTrain Loss: 14.579079 \tmse loss: 3.560746 \tmse2 loss: 2.616160 \tcontrastive loss: 8.402173\n",
      "Saving..\n",
      "Epoch: 97 \tTrain Loss: 14.796694 \tmse loss: 3.485887 \tmse2 loss: 2.637635 \tcontrastive loss: 8.673172\n",
      "Epoch: 98 \tTrain Loss: 14.488470 \tmse loss: 3.575870 \tmse2 loss: 2.794821 \tcontrastive loss: 8.117779\n",
      "Saving..\n",
      "Epoch: 99 \tTrain Loss: 15.611385 \tmse loss: 3.564469 \tmse2 loss: 2.779165 \tcontrastive loss: 9.267751\n",
      "Epoch: 100 \tTrain Loss: 14.719318 \tmse loss: 3.643313 \tmse2 loss: 2.777555 \tcontrastive loss: 8.298451\n",
      "Epoch: 101 \tTrain Loss: 14.491077 \tmse loss: 3.507316 \tmse2 loss: 2.810207 \tcontrastive loss: 8.173554\n",
      "Epoch: 102 \tTrain Loss: 15.109623 \tmse loss: 3.514435 \tmse2 loss: 2.756765 \tcontrastive loss: 8.838423\n",
      "Epoch: 103 \tTrain Loss: 14.842324 \tmse loss: 3.504760 \tmse2 loss: 2.726264 \tcontrastive loss: 8.611300\n",
      "Epoch: 104 \tTrain Loss: 14.652085 \tmse loss: 3.505959 \tmse2 loss: 2.561826 \tcontrastive loss: 8.584300\n",
      "Epoch: 105 \tTrain Loss: 14.777406 \tmse loss: 3.578414 \tmse2 loss: 2.789410 \tcontrastive loss: 8.409582\n",
      "Epoch: 106 \tTrain Loss: 14.051580 \tmse loss: 3.537096 \tmse2 loss: 2.701290 \tcontrastive loss: 7.813195\n",
      "Saving..\n",
      "Epoch: 107 \tTrain Loss: 14.811482 \tmse loss: 3.470840 \tmse2 loss: 2.829644 \tcontrastive loss: 8.510998\n",
      "Epoch: 108 \tTrain Loss: 14.869256 \tmse loss: 3.491014 \tmse2 loss: 2.666719 \tcontrastive loss: 8.711523\n",
      "Epoch: 109 \tTrain Loss: 15.355406 \tmse loss: 3.621450 \tmse2 loss: 2.779483 \tcontrastive loss: 8.954473\n",
      "Epoch: 110 \tTrain Loss: 14.910177 \tmse loss: 3.542949 \tmse2 loss: 2.686082 \tcontrastive loss: 8.681146\n",
      "Epoch: 111 \tTrain Loss: 14.914255 \tmse loss: 3.490516 \tmse2 loss: 2.714393 \tcontrastive loss: 8.709347\n",
      "Epoch: 112 \tTrain Loss: 14.062131 \tmse loss: 3.471350 \tmse2 loss: 2.787437 \tcontrastive loss: 7.803345\n",
      "Epoch: 113 \tTrain Loss: 14.103418 \tmse loss: 3.496930 \tmse2 loss: 2.831547 \tcontrastive loss: 7.774941\n",
      "Epoch: 114 \tTrain Loss: 13.830853 \tmse loss: 3.427673 \tmse2 loss: 2.561725 \tcontrastive loss: 7.841456\n",
      "Saving..\n",
      "Epoch: 115 \tTrain Loss: 14.685494 \tmse loss: 3.436222 \tmse2 loss: 2.767228 \tcontrastive loss: 8.482044\n",
      "Epoch: 116 \tTrain Loss: 13.518854 \tmse loss: 3.489033 \tmse2 loss: 2.720886 \tcontrastive loss: 7.308935\n",
      "Saving..\n",
      "Epoch: 117 \tTrain Loss: 13.784687 \tmse loss: 3.392980 \tmse2 loss: 2.648989 \tcontrastive loss: 7.742718\n",
      "Epoch: 118 \tTrain Loss: 14.324641 \tmse loss: 3.419350 \tmse2 loss: 2.627978 \tcontrastive loss: 8.277313\n",
      "Epoch: 119 \tTrain Loss: 14.261203 \tmse loss: 3.520427 \tmse2 loss: 2.697661 \tcontrastive loss: 8.043115\n",
      "Epoch: 120 \tTrain Loss: 13.566135 \tmse loss: 3.394938 \tmse2 loss: 2.550997 \tcontrastive loss: 7.620199\n",
      "Epoch: 121 \tTrain Loss: 14.152844 \tmse loss: 3.431034 \tmse2 loss: 2.601323 \tcontrastive loss: 8.120487\n",
      "Epoch: 122 \tTrain Loss: 13.594122 \tmse loss: 3.411762 \tmse2 loss: 2.515053 \tcontrastive loss: 7.667307\n",
      "Epoch: 123 \tTrain Loss: 13.161798 \tmse loss: 3.384811 \tmse2 loss: 2.497838 \tcontrastive loss: 7.279149\n",
      "Saving..\n",
      "Epoch: 124 \tTrain Loss: 13.380626 \tmse loss: 3.406325 \tmse2 loss: 2.526468 \tcontrastive loss: 7.447833\n",
      "Epoch: 125 \tTrain Loss: 12.241821 \tmse loss: 3.328803 \tmse2 loss: 2.727926 \tcontrastive loss: 6.185092\n",
      "Saving..\n",
      "Epoch: 126 \tTrain Loss: 12.992935 \tmse loss: 3.319812 \tmse2 loss: 2.406011 \tcontrastive loss: 7.267113\n",
      "Epoch: 127 \tTrain Loss: 13.089196 \tmse loss: 3.286832 \tmse2 loss: 2.455630 \tcontrastive loss: 7.346734\n",
      "Epoch: 128 \tTrain Loss: 13.030543 \tmse loss: 3.322255 \tmse2 loss: 2.478761 \tcontrastive loss: 7.229527\n",
      "Epoch: 129 \tTrain Loss: 12.568963 \tmse loss: 3.248886 \tmse2 loss: 2.473374 \tcontrastive loss: 6.846703\n",
      "Epoch: 130 \tTrain Loss: 13.892749 \tmse loss: 3.411806 \tmse2 loss: 2.592526 \tcontrastive loss: 7.888417\n",
      "Epoch: 131 \tTrain Loss: 13.132863 \tmse loss: 3.327585 \tmse2 loss: 2.614793 \tcontrastive loss: 7.190484\n",
      "Epoch: 132 \tTrain Loss: 13.030594 \tmse loss: 3.322547 \tmse2 loss: 2.588121 \tcontrastive loss: 7.119925\n",
      "Epoch: 133 \tTrain Loss: 13.874838 \tmse loss: 3.358978 \tmse2 loss: 2.703707 \tcontrastive loss: 7.812152\n",
      "Epoch: 134 \tTrain Loss: 12.448583 \tmse loss: 3.337004 \tmse2 loss: 2.457048 \tcontrastive loss: 6.654531\n",
      "Epoch: 135 \tTrain Loss: 12.520640 \tmse loss: 3.362712 \tmse2 loss: 2.518841 \tcontrastive loss: 6.639087\n",
      "Epoch: 136 \tTrain Loss: 12.880340 \tmse loss: 3.323401 \tmse2 loss: 2.509409 \tcontrastive loss: 7.047530\n",
      "Epoch: 137 \tTrain Loss: 12.147421 \tmse loss: 3.299350 \tmse2 loss: 2.483292 \tcontrastive loss: 6.364778\n",
      "Saving..\n",
      "Epoch: 138 \tTrain Loss: 12.504338 \tmse loss: 3.266426 \tmse2 loss: 2.425037 \tcontrastive loss: 6.812875\n",
      "Epoch: 139 \tTrain Loss: 12.484612 \tmse loss: 3.291602 \tmse2 loss: 2.499108 \tcontrastive loss: 6.693903\n",
      "Epoch: 140 \tTrain Loss: 11.985600 \tmse loss: 3.342294 \tmse2 loss: 2.519557 \tcontrastive loss: 6.123749\n",
      "Saving..\n",
      "Epoch: 141 \tTrain Loss: 12.999093 \tmse loss: 3.335187 \tmse2 loss: 2.493017 \tcontrastive loss: 7.170889\n",
      "Epoch: 142 \tTrain Loss: 12.433159 \tmse loss: 3.266155 \tmse2 loss: 2.516824 \tcontrastive loss: 6.650179\n",
      "Epoch: 143 \tTrain Loss: 12.828838 \tmse loss: 3.299320 \tmse2 loss: 2.449662 \tcontrastive loss: 7.079856\n",
      "Epoch: 144 \tTrain Loss: 12.244164 \tmse loss: 3.279225 \tmse2 loss: 2.441270 \tcontrastive loss: 6.523669\n",
      "Epoch: 145 \tTrain Loss: 12.050187 \tmse loss: 3.337811 \tmse2 loss: 2.481026 \tcontrastive loss: 6.231350\n",
      "Epoch: 146 \tTrain Loss: 11.798312 \tmse loss: 3.292836 \tmse2 loss: 2.596413 \tcontrastive loss: 5.909064\n",
      "Saving..\n",
      "Epoch: 147 \tTrain Loss: 12.689224 \tmse loss: 3.290539 \tmse2 loss: 2.547252 \tcontrastive loss: 6.851433\n",
      "Epoch: 148 \tTrain Loss: 11.071978 \tmse loss: 3.230735 \tmse2 loss: 2.373158 \tcontrastive loss: 5.468085\n",
      "Saving..\n",
      "Epoch: 149 \tTrain Loss: 12.855049 \tmse loss: 3.254682 \tmse2 loss: 2.508621 \tcontrastive loss: 7.091747\n",
      "Epoch: 150 \tTrain Loss: 12.200247 \tmse loss: 3.237677 \tmse2 loss: 2.368361 \tcontrastive loss: 6.594209\n",
      "Epoch: 151 \tTrain Loss: 12.005294 \tmse loss: 3.280493 \tmse2 loss: 2.483629 \tcontrastive loss: 6.241173\n",
      "Epoch: 152 \tTrain Loss: 11.886474 \tmse loss: 3.202291 \tmse2 loss: 2.385537 \tcontrastive loss: 6.298646\n",
      "Epoch: 153 \tTrain Loss: 12.005613 \tmse loss: 3.228335 \tmse2 loss: 2.420911 \tcontrastive loss: 6.356367\n",
      "Epoch: 154 \tTrain Loss: 11.840650 \tmse loss: 3.290442 \tmse2 loss: 2.500678 \tcontrastive loss: 6.049530\n",
      "Epoch: 155 \tTrain Loss: 11.345449 \tmse loss: 3.292116 \tmse2 loss: 2.410823 \tcontrastive loss: 5.642510\n",
      "Epoch: 156 \tTrain Loss: 11.653430 \tmse loss: 3.243026 \tmse2 loss: 2.479009 \tcontrastive loss: 5.931395\n",
      "Epoch: 157 \tTrain Loss: 11.489065 \tmse loss: 3.290225 \tmse2 loss: 2.462405 \tcontrastive loss: 5.736434\n",
      "Epoch: 158 \tTrain Loss: 11.338113 \tmse loss: 3.218417 \tmse2 loss: 2.389485 \tcontrastive loss: 5.730210\n",
      "Epoch: 159 \tTrain Loss: 11.035352 \tmse loss: 3.192035 \tmse2 loss: 2.414804 \tcontrastive loss: 5.428513\n",
      "Saving..\n",
      "Epoch: 160 \tTrain Loss: 12.269255 \tmse loss: 3.242580 \tmse2 loss: 2.432795 \tcontrastive loss: 6.593879\n",
      "Epoch: 161 \tTrain Loss: 11.744369 \tmse loss: 3.280433 \tmse2 loss: 2.492155 \tcontrastive loss: 5.971781\n",
      "Epoch: 162 \tTrain Loss: 11.271461 \tmse loss: 3.188860 \tmse2 loss: 2.514625 \tcontrastive loss: 5.567977\n",
      "Epoch: 163 \tTrain Loss: 11.462570 \tmse loss: 3.164613 \tmse2 loss: 2.423420 \tcontrastive loss: 5.874537\n",
      "Epoch: 164 \tTrain Loss: 11.901149 \tmse loss: 3.221439 \tmse2 loss: 2.375939 \tcontrastive loss: 6.303772\n",
      "Epoch: 165 \tTrain Loss: 10.898910 \tmse loss: 3.265442 \tmse2 loss: 2.599325 \tcontrastive loss: 5.034144\n",
      "Saving..\n",
      "Epoch: 166 \tTrain Loss: 10.513891 \tmse loss: 3.149707 \tmse2 loss: 2.412709 \tcontrastive loss: 4.951475\n",
      "Saving..\n",
      "Epoch: 167 \tTrain Loss: 10.577358 \tmse loss: 3.228870 \tmse2 loss: 2.415640 \tcontrastive loss: 4.932848\n",
      "Epoch: 168 \tTrain Loss: 11.296344 \tmse loss: 3.201240 \tmse2 loss: 2.398160 \tcontrastive loss: 5.696944\n",
      "Epoch: 169 \tTrain Loss: 11.896856 \tmse loss: 3.216048 \tmse2 loss: 2.421614 \tcontrastive loss: 6.259195\n",
      "Epoch: 170 \tTrain Loss: 11.819427 \tmse loss: 3.134445 \tmse2 loss: 2.427101 \tcontrastive loss: 6.257881\n",
      "Epoch: 171 \tTrain Loss: 11.041913 \tmse loss: 3.204597 \tmse2 loss: 2.418920 \tcontrastive loss: 5.418395\n",
      "Epoch: 172 \tTrain Loss: 10.250802 \tmse loss: 3.155431 \tmse2 loss: 2.198119 \tcontrastive loss: 4.897252\n",
      "Saving..\n",
      "Epoch: 173 \tTrain Loss: 11.711583 \tmse loss: 3.146871 \tmse2 loss: 2.355857 \tcontrastive loss: 6.208855\n",
      "Epoch: 174 \tTrain Loss: 10.145809 \tmse loss: 3.154124 \tmse2 loss: 2.222157 \tcontrastive loss: 4.769529\n",
      "Saving..\n",
      "Epoch: 175 \tTrain Loss: 10.370207 \tmse loss: 3.197323 \tmse2 loss: 2.469274 \tcontrastive loss: 4.703609\n",
      "Epoch: 176 \tTrain Loss: 10.934999 \tmse loss: 3.147206 \tmse2 loss: 2.265201 \tcontrastive loss: 5.522593\n",
      "Epoch: 177 \tTrain Loss: 10.651649 \tmse loss: 3.248185 \tmse2 loss: 2.468537 \tcontrastive loss: 4.934927\n",
      "Epoch: 178 \tTrain Loss: 9.581173 \tmse loss: 3.125523 \tmse2 loss: 2.235283 \tcontrastive loss: 4.220368\n",
      "Saving..\n",
      "Epoch: 179 \tTrain Loss: 10.314723 \tmse loss: 3.161100 \tmse2 loss: 2.360393 \tcontrastive loss: 4.793230\n",
      "Epoch: 180 \tTrain Loss: 10.225398 \tmse loss: 3.213734 \tmse2 loss: 2.416310 \tcontrastive loss: 4.595355\n",
      "Epoch: 181 \tTrain Loss: 11.196775 \tmse loss: 3.211998 \tmse2 loss: 2.417513 \tcontrastive loss: 5.567263\n",
      "Epoch: 182 \tTrain Loss: 10.032872 \tmse loss: 3.158726 \tmse2 loss: 2.417207 \tcontrastive loss: 4.456939\n",
      "Epoch: 183 \tTrain Loss: 9.539866 \tmse loss: 3.130145 \tmse2 loss: 2.370370 \tcontrastive loss: 4.039350\n",
      "Saving..\n",
      "Epoch: 184 \tTrain Loss: 8.858028 \tmse loss: 3.068066 \tmse2 loss: 2.066896 \tcontrastive loss: 3.723066\n",
      "Saving..\n",
      "Epoch: 185 \tTrain Loss: 9.312400 \tmse loss: 3.087005 \tmse2 loss: 2.421293 \tcontrastive loss: 3.804102\n",
      "Epoch: 186 \tTrain Loss: 9.768116 \tmse loss: 3.053195 \tmse2 loss: 2.356379 \tcontrastive loss: 4.358542\n",
      "Epoch: 187 \tTrain Loss: 9.962428 \tmse loss: 3.167085 \tmse2 loss: 2.290119 \tcontrastive loss: 4.505224\n",
      "Epoch: 188 \tTrain Loss: 10.677593 \tmse loss: 3.141647 \tmse2 loss: 2.243967 \tcontrastive loss: 5.291979\n",
      "Epoch: 189 \tTrain Loss: 9.261577 \tmse loss: 3.100241 \tmse2 loss: 2.265121 \tcontrastive loss: 3.896216\n",
      "Epoch: 190 \tTrain Loss: 8.754132 \tmse loss: 3.040176 \tmse2 loss: 2.150194 \tcontrastive loss: 3.563763\n",
      "Saving..\n",
      "Epoch: 191 \tTrain Loss: 9.439949 \tmse loss: 3.030187 \tmse2 loss: 2.149649 \tcontrastive loss: 4.260113\n",
      "Epoch: 192 \tTrain Loss: 8.526669 \tmse loss: 3.006799 \tmse2 loss: 2.158143 \tcontrastive loss: 3.361728\n",
      "Saving..\n",
      "Epoch: 193 \tTrain Loss: 9.419723 \tmse loss: 3.044509 \tmse2 loss: 2.312985 \tcontrastive loss: 4.062230\n",
      "Epoch: 194 \tTrain Loss: 9.727604 \tmse loss: 3.013696 \tmse2 loss: 2.379436 \tcontrastive loss: 4.334472\n",
      "Epoch: 195 \tTrain Loss: 10.371664 \tmse loss: 3.084992 \tmse2 loss: 2.340819 \tcontrastive loss: 4.945853\n",
      "Epoch: 196 \tTrain Loss: 10.094493 \tmse loss: 3.140700 \tmse2 loss: 2.252416 \tcontrastive loss: 4.701377\n",
      "Epoch: 197 \tTrain Loss: 9.998744 \tmse loss: 3.144408 \tmse2 loss: 2.286842 \tcontrastive loss: 4.567495\n",
      "Epoch: 198 \tTrain Loss: 8.583168 \tmse loss: 3.093309 \tmse2 loss: 2.216980 \tcontrastive loss: 3.272878\n",
      "Epoch: 199 \tTrain Loss: 9.225677 \tmse loss: 3.030781 \tmse2 loss: 2.155279 \tcontrastive loss: 4.039617\n",
      "Epoch: 200 \tTrain Loss: 10.536042 \tmse loss: 3.039184 \tmse2 loss: 2.278837 \tcontrastive loss: 5.218021\n",
      "Epoch: 201 \tTrain Loss: 9.668246 \tmse loss: 3.017260 \tmse2 loss: 2.197502 \tcontrastive loss: 4.453484\n",
      "Epoch: 202 \tTrain Loss: 9.966431 \tmse loss: 3.053265 \tmse2 loss: 2.299792 \tcontrastive loss: 4.613374\n",
      "Epoch: 203 \tTrain Loss: 8.885345 \tmse loss: 3.046419 \tmse2 loss: 2.218444 \tcontrastive loss: 3.620482\n",
      "Epoch: 204 \tTrain Loss: 9.253597 \tmse loss: 3.017653 \tmse2 loss: 2.239738 \tcontrastive loss: 3.996205\n",
      "Epoch: 205 \tTrain Loss: 10.172496 \tmse loss: 3.028661 \tmse2 loss: 2.412968 \tcontrastive loss: 4.730867\n",
      "Epoch: 206 \tTrain Loss: 8.623419 \tmse loss: 3.016808 \tmse2 loss: 2.321914 \tcontrastive loss: 3.284697\n",
      "Epoch: 207 \tTrain Loss: 9.169692 \tmse loss: 3.003031 \tmse2 loss: 2.113524 \tcontrastive loss: 4.053138\n",
      "Epoch: 208 \tTrain Loss: 8.604570 \tmse loss: 2.969024 \tmse2 loss: 2.270407 \tcontrastive loss: 3.365140\n",
      "Epoch: 209 \tTrain Loss: 8.793863 \tmse loss: 2.968684 \tmse2 loss: 2.314900 \tcontrastive loss: 3.510279\n",
      "Epoch: 210 \tTrain Loss: 9.416178 \tmse loss: 2.988259 \tmse2 loss: 2.190356 \tcontrastive loss: 4.237562\n",
      "Epoch: 211 \tTrain Loss: 7.975177 \tmse loss: 2.984973 \tmse2 loss: 2.222477 \tcontrastive loss: 2.767726\n",
      "Saving..\n",
      "Epoch: 212 \tTrain Loss: 9.074223 \tmse loss: 2.984562 \tmse2 loss: 2.057382 \tcontrastive loss: 4.032279\n",
      "Epoch: 213 \tTrain Loss: 8.388336 \tmse loss: 2.920406 \tmse2 loss: 2.120784 \tcontrastive loss: 3.347146\n",
      "Epoch: 214 \tTrain Loss: 8.767356 \tmse loss: 3.055404 \tmse2 loss: 2.194845 \tcontrastive loss: 3.517107\n",
      "Epoch: 215 \tTrain Loss: 10.098250 \tmse loss: 3.006637 \tmse2 loss: 2.209426 \tcontrastive loss: 4.882186\n",
      "Epoch: 216 \tTrain Loss: 9.221543 \tmse loss: 2.970314 \tmse2 loss: 2.153475 \tcontrastive loss: 4.097754\n",
      "Epoch: 217 \tTrain Loss: 8.440640 \tmse loss: 3.013979 \tmse2 loss: 2.170860 \tcontrastive loss: 3.255801\n",
      "Epoch: 218 \tTrain Loss: 8.922101 \tmse loss: 3.051914 \tmse2 loss: 2.236995 \tcontrastive loss: 3.633192\n",
      "Epoch: 219 \tTrain Loss: 9.003207 \tmse loss: 3.011049 \tmse2 loss: 2.337514 \tcontrastive loss: 3.654645\n",
      "Epoch: 220 \tTrain Loss: 9.188158 \tmse loss: 2.989638 \tmse2 loss: 2.214581 \tcontrastive loss: 3.983940\n",
      "Epoch: 221 \tTrain Loss: 9.343896 \tmse loss: 2.987084 \tmse2 loss: 2.309786 \tcontrastive loss: 4.047026\n",
      "Epoch: 222 \tTrain Loss: 9.870287 \tmse loss: 3.014222 \tmse2 loss: 2.216262 \tcontrastive loss: 4.639803\n",
      "Epoch: 223 \tTrain Loss: 8.453520 \tmse loss: 2.921831 \tmse2 loss: 2.149472 \tcontrastive loss: 3.382217\n",
      "Epoch: 224 \tTrain Loss: 8.903801 \tmse loss: 2.903328 \tmse2 loss: 1.962001 \tcontrastive loss: 4.038472\n",
      "Epoch: 225 \tTrain Loss: 8.497874 \tmse loss: 2.854452 \tmse2 loss: 2.107786 \tcontrastive loss: 3.535635\n",
      "Epoch: 226 \tTrain Loss: 8.267698 \tmse loss: 2.932492 \tmse2 loss: 2.180262 \tcontrastive loss: 3.154944\n",
      "Epoch: 227 \tTrain Loss: 8.627258 \tmse loss: 2.976362 \tmse2 loss: 2.185962 \tcontrastive loss: 3.464934\n",
      "Epoch: 228 \tTrain Loss: 10.049414 \tmse loss: 2.966321 \tmse2 loss: 2.315968 \tcontrastive loss: 4.767124\n",
      "Epoch: 229 \tTrain Loss: 8.827348 \tmse loss: 2.947811 \tmse2 loss: 2.205933 \tcontrastive loss: 3.673605\n",
      "Epoch: 230 \tTrain Loss: 8.536905 \tmse loss: 2.936011 \tmse2 loss: 2.070518 \tcontrastive loss: 3.530375\n",
      "Epoch: 231 \tTrain Loss: 8.566473 \tmse loss: 2.889740 \tmse2 loss: 2.162084 \tcontrastive loss: 3.514649\n",
      "Epoch: 232 \tTrain Loss: 8.021631 \tmse loss: 2.885446 \tmse2 loss: 2.192168 \tcontrastive loss: 2.944017\n",
      "Epoch: 233 \tTrain Loss: 8.032979 \tmse loss: 2.810984 \tmse2 loss: 2.019464 \tcontrastive loss: 3.202531\n",
      "Epoch: 234 \tTrain Loss: 7.835870 \tmse loss: 2.842796 \tmse2 loss: 2.092508 \tcontrastive loss: 2.900565\n",
      "Saving..\n",
      "Epoch: 235 \tTrain Loss: 8.236624 \tmse loss: 2.837629 \tmse2 loss: 1.918163 \tcontrastive loss: 3.480832\n",
      "Epoch: 236 \tTrain Loss: 7.719623 \tmse loss: 2.853071 \tmse2 loss: 2.148701 \tcontrastive loss: 2.717851\n",
      "Saving..\n",
      "Epoch: 237 \tTrain Loss: 7.896325 \tmse loss: 2.880947 \tmse2 loss: 2.065318 \tcontrastive loss: 2.950060\n",
      "Epoch: 238 \tTrain Loss: 8.645561 \tmse loss: 2.927988 \tmse2 loss: 2.199337 \tcontrastive loss: 3.518235\n",
      "Epoch: 239 \tTrain Loss: 8.065948 \tmse loss: 2.887309 \tmse2 loss: 2.192654 \tcontrastive loss: 2.985985\n",
      "Epoch: 240 \tTrain Loss: 8.207497 \tmse loss: 2.905918 \tmse2 loss: 2.115619 \tcontrastive loss: 3.185960\n",
      "Epoch: 241 \tTrain Loss: 8.217911 \tmse loss: 2.883144 \tmse2 loss: 2.148206 \tcontrastive loss: 3.186561\n",
      "Epoch: 242 \tTrain Loss: 9.843110 \tmse loss: 2.924444 \tmse2 loss: 2.172961 \tcontrastive loss: 4.745705\n",
      "Epoch: 243 \tTrain Loss: 8.932131 \tmse loss: 2.954279 \tmse2 loss: 2.034469 \tcontrastive loss: 3.943384\n",
      "Epoch: 244 \tTrain Loss: 8.658808 \tmse loss: 2.888132 \tmse2 loss: 2.024706 \tcontrastive loss: 3.745969\n",
      "Epoch: 245 \tTrain Loss: 8.312803 \tmse loss: 2.886391 \tmse2 loss: 2.098074 \tcontrastive loss: 3.328338\n",
      "Epoch: 246 \tTrain Loss: 8.838896 \tmse loss: 2.898248 \tmse2 loss: 2.114527 \tcontrastive loss: 3.826122\n",
      "Epoch: 247 \tTrain Loss: 8.172327 \tmse loss: 2.833860 \tmse2 loss: 2.144044 \tcontrastive loss: 3.194423\n",
      "Epoch: 248 \tTrain Loss: 8.516225 \tmse loss: 2.927550 \tmse2 loss: 2.103725 \tcontrastive loss: 3.484950\n",
      "Epoch: 249 \tTrain Loss: 9.095254 \tmse loss: 2.827804 \tmse2 loss: 2.067411 \tcontrastive loss: 4.200038\n",
      "Epoch: 250 \tTrain Loss: 8.384719 \tmse loss: 2.840135 \tmse2 loss: 1.866463 \tcontrastive loss: 3.678121\n",
      "Epoch: 251 \tTrain Loss: 8.663233 \tmse loss: 2.816172 \tmse2 loss: 2.139484 \tcontrastive loss: 3.707577\n",
      "Epoch: 252 \tTrain Loss: 8.579294 \tmse loss: 2.879648 \tmse2 loss: 2.016830 \tcontrastive loss: 3.682816\n",
      "Epoch: 253 \tTrain Loss: 8.036555 \tmse loss: 2.822866 \tmse2 loss: 2.183839 \tcontrastive loss: 3.029850\n",
      "Epoch: 254 \tTrain Loss: 7.651388 \tmse loss: 2.885248 \tmse2 loss: 2.122234 \tcontrastive loss: 2.643906\n",
      "Saving..\n",
      "Epoch: 255 \tTrain Loss: 8.022000 \tmse loss: 2.830205 \tmse2 loss: 2.159928 \tcontrastive loss: 3.031867\n",
      "Epoch: 256 \tTrain Loss: 7.689528 \tmse loss: 2.769202 \tmse2 loss: 2.149102 \tcontrastive loss: 2.771224\n",
      "Epoch: 257 \tTrain Loss: 8.016837 \tmse loss: 2.742826 \tmse2 loss: 2.127747 \tcontrastive loss: 3.146263\n",
      "Epoch: 258 \tTrain Loss: 6.929327 \tmse loss: 2.787602 \tmse2 loss: 2.118650 \tcontrastive loss: 2.023076\n",
      "Saving..\n",
      "Epoch: 259 \tTrain Loss: 7.274609 \tmse loss: 2.821813 \tmse2 loss: 2.062528 \tcontrastive loss: 2.390267\n",
      "Epoch: 260 \tTrain Loss: 8.442237 \tmse loss: 2.813971 \tmse2 loss: 2.018481 \tcontrastive loss: 3.609785\n",
      "Epoch: 261 \tTrain Loss: 8.683148 \tmse loss: 2.854232 \tmse2 loss: 2.080096 \tcontrastive loss: 3.748820\n",
      "Epoch: 262 \tTrain Loss: 9.328882 \tmse loss: 2.809121 \tmse2 loss: 2.105108 \tcontrastive loss: 4.414652\n",
      "Epoch: 263 \tTrain Loss: 7.623756 \tmse loss: 2.846928 \tmse2 loss: 2.110828 \tcontrastive loss: 2.665999\n",
      "Epoch: 264 \tTrain Loss: 6.799636 \tmse loss: 2.745243 \tmse2 loss: 1.971661 \tcontrastive loss: 2.082732\n",
      "Saving..\n",
      "Epoch: 265 \tTrain Loss: 7.147693 \tmse loss: 2.785292 \tmse2 loss: 1.965398 \tcontrastive loss: 2.397004\n",
      "Epoch: 266 \tTrain Loss: 7.576585 \tmse loss: 2.749028 \tmse2 loss: 1.975583 \tcontrastive loss: 2.851974\n",
      "Epoch: 267 \tTrain Loss: 6.921990 \tmse loss: 2.720336 \tmse2 loss: 1.888668 \tcontrastive loss: 2.312986\n",
      "Epoch: 268 \tTrain Loss: 7.975082 \tmse loss: 2.714893 \tmse2 loss: 2.017640 \tcontrastive loss: 3.242550\n",
      "Epoch: 269 \tTrain Loss: 7.568880 \tmse loss: 2.750910 \tmse2 loss: 1.930378 \tcontrastive loss: 2.887592\n",
      "Epoch: 270 \tTrain Loss: 7.190619 \tmse loss: 2.749065 \tmse2 loss: 1.908968 \tcontrastive loss: 2.532586\n",
      "Epoch: 271 \tTrain Loss: 7.057957 \tmse loss: 2.732979 \tmse2 loss: 1.929783 \tcontrastive loss: 2.395195\n",
      "Epoch: 272 \tTrain Loss: 7.387248 \tmse loss: 2.702610 \tmse2 loss: 1.902452 \tcontrastive loss: 2.782186\n",
      "Epoch: 273 \tTrain Loss: 6.800719 \tmse loss: 2.659071 \tmse2 loss: 1.837084 \tcontrastive loss: 2.304564\n",
      "Epoch: 274 \tTrain Loss: 6.735952 \tmse loss: 2.673074 \tmse2 loss: 1.954323 \tcontrastive loss: 2.108555\n",
      "Saving..\n",
      "Epoch: 275 \tTrain Loss: 6.722093 \tmse loss: 2.645257 \tmse2 loss: 1.945218 \tcontrastive loss: 2.131618\n",
      "Saving..\n",
      "Epoch: 276 \tTrain Loss: 8.089850 \tmse loss: 2.765341 \tmse2 loss: 2.171035 \tcontrastive loss: 3.153474\n",
      "Epoch: 277 \tTrain Loss: 8.170208 \tmse loss: 2.731793 \tmse2 loss: 1.918117 \tcontrastive loss: 3.520298\n",
      "Epoch: 278 \tTrain Loss: 8.880834 \tmse loss: 2.772892 \tmse2 loss: 2.089639 \tcontrastive loss: 4.018303\n",
      "Epoch: 279 \tTrain Loss: 8.423000 \tmse loss: 2.762325 \tmse2 loss: 2.112469 \tcontrastive loss: 3.548206\n",
      "Epoch: 280 \tTrain Loss: 8.891789 \tmse loss: 2.809564 \tmse2 loss: 2.058146 \tcontrastive loss: 4.024078\n",
      "Epoch: 281 \tTrain Loss: 7.685219 \tmse loss: 2.789795 \tmse2 loss: 2.021947 \tcontrastive loss: 2.873476\n",
      "Epoch: 282 \tTrain Loss: 7.405388 \tmse loss: 2.714566 \tmse2 loss: 1.981650 \tcontrastive loss: 2.709172\n",
      "Epoch: 283 \tTrain Loss: 7.966386 \tmse loss: 2.686102 \tmse2 loss: 1.961466 \tcontrastive loss: 3.318818\n",
      "Epoch: 284 \tTrain Loss: 7.694096 \tmse loss: 2.695553 \tmse2 loss: 1.950843 \tcontrastive loss: 3.047699\n",
      "Epoch: 285 \tTrain Loss: 6.213353 \tmse loss: 2.707513 \tmse2 loss: 1.988583 \tcontrastive loss: 1.517257\n",
      "Saving..\n",
      "Epoch: 286 \tTrain Loss: 7.492728 \tmse loss: 2.698136 \tmse2 loss: 1.971075 \tcontrastive loss: 2.823517\n",
      "Epoch: 287 \tTrain Loss: 6.991769 \tmse loss: 2.667238 \tmse2 loss: 1.947628 \tcontrastive loss: 2.376903\n",
      "Epoch: 288 \tTrain Loss: 7.449475 \tmse loss: 2.741504 \tmse2 loss: 1.990477 \tcontrastive loss: 2.717494\n",
      "Epoch: 289 \tTrain Loss: 7.956848 \tmse loss: 2.726243 \tmse2 loss: 2.020733 \tcontrastive loss: 3.209873\n",
      "Epoch: 290 \tTrain Loss: 7.689898 \tmse loss: 2.752615 \tmse2 loss: 2.008466 \tcontrastive loss: 2.928817\n",
      "Epoch: 291 \tTrain Loss: 7.432463 \tmse loss: 2.718013 \tmse2 loss: 2.021038 \tcontrastive loss: 2.693412\n",
      "Epoch: 292 \tTrain Loss: 6.614970 \tmse loss: 2.608823 \tmse2 loss: 2.087742 \tcontrastive loss: 1.918405\n",
      "Epoch: 293 \tTrain Loss: 7.379968 \tmse loss: 2.680559 \tmse2 loss: 1.974556 \tcontrastive loss: 2.724853\n",
      "Epoch: 294 \tTrain Loss: 6.600210 \tmse loss: 2.681293 \tmse2 loss: 1.905734 \tcontrastive loss: 2.013182\n",
      "Epoch: 295 \tTrain Loss: 7.525638 \tmse loss: 2.660785 \tmse2 loss: 1.865328 \tcontrastive loss: 2.999525\n",
      "Epoch: 296 \tTrain Loss: 7.985742 \tmse loss: 2.666407 \tmse2 loss: 1.930528 \tcontrastive loss: 3.388807\n",
      "Epoch: 297 \tTrain Loss: 7.420467 \tmse loss: 2.627727 \tmse2 loss: 2.032037 \tcontrastive loss: 2.760704\n",
      "Epoch: 298 \tTrain Loss: 7.982999 \tmse loss: 2.665179 \tmse2 loss: 2.085560 \tcontrastive loss: 3.232259\n",
      "Epoch: 299 \tTrain Loss: 8.082659 \tmse loss: 2.713296 \tmse2 loss: 2.016981 \tcontrastive loss: 3.352382\n",
      "Epoch: 300 \tTrain Loss: 6.208360 \tmse loss: 2.633140 \tmse2 loss: 1.890350 \tcontrastive loss: 1.684869\n",
      "Saving..\n",
      "Epoch: 301 \tTrain Loss: 6.236030 \tmse loss: 2.605399 \tmse2 loss: 1.884911 \tcontrastive loss: 1.745720\n",
      "Epoch: 302 \tTrain Loss: 5.358610 \tmse loss: 2.555791 \tmse2 loss: 1.766971 \tcontrastive loss: 1.035848\n",
      "Saving..\n",
      "Epoch: 303 \tTrain Loss: 5.820138 \tmse loss: 2.532553 \tmse2 loss: 1.945315 \tcontrastive loss: 1.342270\n",
      "Epoch: 304 \tTrain Loss: 6.550326 \tmse loss: 2.604845 \tmse2 loss: 1.794931 \tcontrastive loss: 2.150551\n",
      "Epoch: 305 \tTrain Loss: 7.114157 \tmse loss: 2.603936 \tmse2 loss: 1.798541 \tcontrastive loss: 2.711680\n",
      "Epoch: 306 \tTrain Loss: 7.319805 \tmse loss: 2.593874 \tmse2 loss: 1.914364 \tcontrastive loss: 2.811567\n",
      "Epoch: 307 \tTrain Loss: 7.547778 \tmse loss: 2.573362 \tmse2 loss: 1.955926 \tcontrastive loss: 3.018490\n",
      "Epoch: 308 \tTrain Loss: 7.993722 \tmse loss: 2.549942 \tmse2 loss: 1.875428 \tcontrastive loss: 3.568352\n",
      "Epoch: 309 \tTrain Loss: 8.313106 \tmse loss: 2.615448 \tmse2 loss: 2.040119 \tcontrastive loss: 3.657539\n",
      "Epoch: 310 \tTrain Loss: 6.354185 \tmse loss: 2.614449 \tmse2 loss: 1.951462 \tcontrastive loss: 1.788275\n",
      "Epoch: 311 \tTrain Loss: 5.977613 \tmse loss: 2.540190 \tmse2 loss: 1.897395 \tcontrastive loss: 1.540028\n",
      "Epoch: 312 \tTrain Loss: 6.211431 \tmse loss: 2.557765 \tmse2 loss: 1.925874 \tcontrastive loss: 1.727792\n",
      "Epoch: 313 \tTrain Loss: 6.448001 \tmse loss: 2.537254 \tmse2 loss: 1.805979 \tcontrastive loss: 2.104769\n",
      "Epoch: 314 \tTrain Loss: 6.485084 \tmse loss: 2.576952 \tmse2 loss: 1.835804 \tcontrastive loss: 2.072328\n",
      "Epoch: 315 \tTrain Loss: 7.269026 \tmse loss: 2.551375 \tmse2 loss: 1.831741 \tcontrastive loss: 2.885910\n",
      "Epoch: 316 \tTrain Loss: 6.889970 \tmse loss: 2.581967 \tmse2 loss: 1.902998 \tcontrastive loss: 2.405005\n",
      "Epoch: 317 \tTrain Loss: 7.102705 \tmse loss: 2.626347 \tmse2 loss: 1.832972 \tcontrastive loss: 2.643386\n",
      "Epoch: 318 \tTrain Loss: 6.878016 \tmse loss: 2.622901 \tmse2 loss: 1.876093 \tcontrastive loss: 2.379022\n",
      "Epoch: 319 \tTrain Loss: 6.838491 \tmse loss: 2.580533 \tmse2 loss: 1.915387 \tcontrastive loss: 2.342571\n",
      "Epoch: 320 \tTrain Loss: 7.302466 \tmse loss: 2.569105 \tmse2 loss: 1.768990 \tcontrastive loss: 2.964371\n",
      "Epoch: 321 \tTrain Loss: 7.343477 \tmse loss: 2.655446 \tmse2 loss: 2.002872 \tcontrastive loss: 2.685159\n",
      "Epoch: 322 \tTrain Loss: 7.410195 \tmse loss: 2.598253 \tmse2 loss: 1.751118 \tcontrastive loss: 3.060824\n",
      "Epoch: 323 \tTrain Loss: 6.593378 \tmse loss: 2.638989 \tmse2 loss: 1.844737 \tcontrastive loss: 2.109652\n",
      "Epoch: 324 \tTrain Loss: 6.184253 \tmse loss: 2.548542 \tmse2 loss: 1.647252 \tcontrastive loss: 1.988459\n",
      "Epoch: 325 \tTrain Loss: 6.392998 \tmse loss: 2.551573 \tmse2 loss: 1.872023 \tcontrastive loss: 1.969402\n",
      "Epoch: 326 \tTrain Loss: 6.130800 \tmse loss: 2.497003 \tmse2 loss: 1.887715 \tcontrastive loss: 1.746082\n",
      "Epoch: 327 \tTrain Loss: 6.767198 \tmse loss: 2.528082 \tmse2 loss: 1.822969 \tcontrastive loss: 2.416146\n",
      "Epoch: 328 \tTrain Loss: 6.924613 \tmse loss: 2.631248 \tmse2 loss: 1.881463 \tcontrastive loss: 2.411901\n",
      "Epoch: 329 \tTrain Loss: 6.367931 \tmse loss: 2.562305 \tmse2 loss: 1.859952 \tcontrastive loss: 1.945674\n",
      "Epoch: 330 \tTrain Loss: 6.379783 \tmse loss: 2.549779 \tmse2 loss: 1.860117 \tcontrastive loss: 1.969887\n",
      "Epoch: 331 \tTrain Loss: 7.584686 \tmse loss: 2.544411 \tmse2 loss: 1.850777 \tcontrastive loss: 3.189498\n",
      "Epoch: 332 \tTrain Loss: 6.175714 \tmse loss: 2.476329 \tmse2 loss: 1.836451 \tcontrastive loss: 1.862934\n",
      "Epoch: 333 \tTrain Loss: 7.536760 \tmse loss: 2.504507 \tmse2 loss: 1.864887 \tcontrastive loss: 3.167366\n",
      "Epoch: 334 \tTrain Loss: 7.952297 \tmse loss: 2.516955 \tmse2 loss: 1.855716 \tcontrastive loss: 3.579626\n",
      "Epoch: 335 \tTrain Loss: 7.374060 \tmse loss: 2.552061 \tmse2 loss: 2.004756 \tcontrastive loss: 2.817243\n",
      "Epoch: 336 \tTrain Loss: 7.280741 \tmse loss: 2.473153 \tmse2 loss: 1.901819 \tcontrastive loss: 2.905768\n",
      "Epoch: 337 \tTrain Loss: 6.888965 \tmse loss: 2.519462 \tmse2 loss: 1.876501 \tcontrastive loss: 2.493003\n",
      "Epoch: 338 \tTrain Loss: 6.401433 \tmse loss: 2.517311 \tmse2 loss: 1.879462 \tcontrastive loss: 2.004660\n",
      "Epoch: 339 \tTrain Loss: 6.316760 \tmse loss: 2.452581 \tmse2 loss: 1.745804 \tcontrastive loss: 2.118375\n",
      "Epoch: 340 \tTrain Loss: 6.228962 \tmse loss: 2.457275 \tmse2 loss: 1.752668 \tcontrastive loss: 2.019019\n",
      "Epoch: 341 \tTrain Loss: 7.449154 \tmse loss: 2.508228 \tmse2 loss: 1.818552 \tcontrastive loss: 3.122374\n",
      "Epoch: 342 \tTrain Loss: 6.380791 \tmse loss: 2.477737 \tmse2 loss: 1.776508 \tcontrastive loss: 2.126546\n",
      "Epoch: 343 \tTrain Loss: 7.989702 \tmse loss: 2.502249 \tmse2 loss: 1.851537 \tcontrastive loss: 3.635916\n",
      "Epoch: 344 \tTrain Loss: 7.905750 \tmse loss: 2.423789 \tmse2 loss: 1.752107 \tcontrastive loss: 3.729854\n",
      "Epoch: 345 \tTrain Loss: 6.017907 \tmse loss: 2.459292 \tmse2 loss: 1.766398 \tcontrastive loss: 1.792217\n",
      "Epoch: 346 \tTrain Loss: 5.937991 \tmse loss: 2.511315 \tmse2 loss: 1.778194 \tcontrastive loss: 1.648481\n",
      "Epoch: 347 \tTrain Loss: 5.791931 \tmse loss: 2.424971 \tmse2 loss: 1.702752 \tcontrastive loss: 1.664208\n",
      "Epoch: 348 \tTrain Loss: 6.557639 \tmse loss: 2.429964 \tmse2 loss: 1.768130 \tcontrastive loss: 2.359545\n",
      "Epoch: 349 \tTrain Loss: 6.429191 \tmse loss: 2.484894 \tmse2 loss: 1.778678 \tcontrastive loss: 2.165619\n",
      "Epoch: 350 \tTrain Loss: 6.416030 \tmse loss: 2.454407 \tmse2 loss: 1.802394 \tcontrastive loss: 2.159230\n",
      "Epoch: 351 \tTrain Loss: 6.428771 \tmse loss: 2.492312 \tmse2 loss: 1.979725 \tcontrastive loss: 1.956735\n",
      "Epoch: 352 \tTrain Loss: 6.936094 \tmse loss: 2.468199 \tmse2 loss: 1.881351 \tcontrastive loss: 2.586544\n",
      "Epoch: 353 \tTrain Loss: 6.205315 \tmse loss: 2.439821 \tmse2 loss: 1.731986 \tcontrastive loss: 2.033509\n",
      "Epoch: 354 \tTrain Loss: 6.446189 \tmse loss: 2.466354 \tmse2 loss: 1.766145 \tcontrastive loss: 2.213690\n",
      "Epoch: 355 \tTrain Loss: 6.964069 \tmse loss: 2.495309 \tmse2 loss: 1.717795 \tcontrastive loss: 2.750964\n",
      "Epoch: 356 \tTrain Loss: 6.431154 \tmse loss: 2.502071 \tmse2 loss: 1.836197 \tcontrastive loss: 2.092886\n",
      "Epoch: 357 \tTrain Loss: 7.345481 \tmse loss: 2.529237 \tmse2 loss: 1.867786 \tcontrastive loss: 2.948459\n",
      "Epoch: 358 \tTrain Loss: 5.952518 \tmse loss: 2.449403 \tmse2 loss: 1.767551 \tcontrastive loss: 1.735564\n",
      "Epoch: 359 \tTrain Loss: 7.341327 \tmse loss: 2.430245 \tmse2 loss: 1.788991 \tcontrastive loss: 3.122090\n",
      "Epoch: 360 \tTrain Loss: 6.316432 \tmse loss: 2.391650 \tmse2 loss: 1.842293 \tcontrastive loss: 2.082489\n",
      "Epoch: 361 \tTrain Loss: 6.259486 \tmse loss: 2.374607 \tmse2 loss: 1.712215 \tcontrastive loss: 2.172665\n",
      "Epoch: 362 \tTrain Loss: 6.449286 \tmse loss: 2.366124 \tmse2 loss: 1.650446 \tcontrastive loss: 2.432716\n",
      "Epoch: 363 \tTrain Loss: 6.100754 \tmse loss: 2.386814 \tmse2 loss: 1.783400 \tcontrastive loss: 1.930540\n",
      "Epoch: 364 \tTrain Loss: 6.043726 \tmse loss: 2.353972 \tmse2 loss: 1.771355 \tcontrastive loss: 1.918400\n",
      "Epoch: 365 \tTrain Loss: 5.224569 \tmse loss: 2.346766 \tmse2 loss: 1.752244 \tcontrastive loss: 1.125559\n",
      "Saving..\n",
      "Epoch: 366 \tTrain Loss: 5.399742 \tmse loss: 2.333112 \tmse2 loss: 1.714560 \tcontrastive loss: 1.352070\n",
      "Epoch: 367 \tTrain Loss: 5.190177 \tmse loss: 2.308926 \tmse2 loss: 1.651608 \tcontrastive loss: 1.229642\n",
      "Saving..\n",
      "Epoch: 368 \tTrain Loss: 5.074550 \tmse loss: 2.300439 \tmse2 loss: 1.554343 \tcontrastive loss: 1.219769\n",
      "Saving..\n",
      "Epoch: 369 \tTrain Loss: 6.321382 \tmse loss: 2.380343 \tmse2 loss: 1.801791 \tcontrastive loss: 2.139248\n",
      "Epoch: 370 \tTrain Loss: 7.602118 \tmse loss: 2.385130 \tmse2 loss: 1.750579 \tcontrastive loss: 3.466408\n",
      "Epoch: 371 \tTrain Loss: 7.295228 \tmse loss: 2.457669 \tmse2 loss: 1.733658 \tcontrastive loss: 3.103901\n",
      "Epoch: 372 \tTrain Loss: 6.901686 \tmse loss: 2.407592 \tmse2 loss: 1.759186 \tcontrastive loss: 2.734909\n",
      "Epoch: 373 \tTrain Loss: 6.952035 \tmse loss: 2.412226 \tmse2 loss: 1.660180 \tcontrastive loss: 2.879629\n",
      "Epoch: 374 \tTrain Loss: 7.348961 \tmse loss: 2.422904 \tmse2 loss: 1.915866 \tcontrastive loss: 3.010191\n",
      "Epoch: 375 \tTrain Loss: 5.599277 \tmse loss: 2.370718 \tmse2 loss: 1.710335 \tcontrastive loss: 1.518224\n",
      "Epoch: 376 \tTrain Loss: 5.292972 \tmse loss: 2.341215 \tmse2 loss: 1.707595 \tcontrastive loss: 1.244162\n",
      "Epoch: 377 \tTrain Loss: 5.796518 \tmse loss: 2.360107 \tmse2 loss: 1.581003 \tcontrastive loss: 1.855408\n",
      "Epoch: 378 \tTrain Loss: 5.610911 \tmse loss: 2.375023 \tmse2 loss: 1.663713 \tcontrastive loss: 1.572175\n",
      "Epoch: 379 \tTrain Loss: 5.301226 \tmse loss: 2.306364 \tmse2 loss: 1.678094 \tcontrastive loss: 1.316768\n",
      "Epoch: 380 \tTrain Loss: 6.018110 \tmse loss: 2.318445 \tmse2 loss: 1.704347 \tcontrastive loss: 1.995318\n",
      "Epoch: 381 \tTrain Loss: 5.895434 \tmse loss: 2.340727 \tmse2 loss: 1.763519 \tcontrastive loss: 1.791188\n",
      "Epoch: 382 \tTrain Loss: 5.154996 \tmse loss: 2.346471 \tmse2 loss: 1.491841 \tcontrastive loss: 1.316685\n",
      "Epoch: 383 \tTrain Loss: 5.752910 \tmse loss: 2.358398 \tmse2 loss: 1.650332 \tcontrastive loss: 1.744181\n",
      "Epoch: 384 \tTrain Loss: 6.116124 \tmse loss: 2.340975 \tmse2 loss: 1.665155 \tcontrastive loss: 2.109994\n",
      "Epoch: 385 \tTrain Loss: 6.801973 \tmse loss: 2.415093 \tmse2 loss: 1.707946 \tcontrastive loss: 2.678934\n",
      "Epoch: 386 \tTrain Loss: 5.890288 \tmse loss: 2.396595 \tmse2 loss: 1.716699 \tcontrastive loss: 1.776994\n",
      "Epoch: 387 \tTrain Loss: 5.692201 \tmse loss: 2.351694 \tmse2 loss: 1.682332 \tcontrastive loss: 1.658175\n",
      "Epoch: 388 \tTrain Loss: 5.402407 \tmse loss: 2.272800 \tmse2 loss: 1.627980 \tcontrastive loss: 1.501627\n",
      "Epoch: 389 \tTrain Loss: 6.086733 \tmse loss: 2.274919 \tmse2 loss: 1.731789 \tcontrastive loss: 2.080025\n",
      "Epoch: 390 \tTrain Loss: 6.131059 \tmse loss: 2.351556 \tmse2 loss: 1.737315 \tcontrastive loss: 2.042188\n",
      "Epoch: 391 \tTrain Loss: 5.441539 \tmse loss: 2.313247 \tmse2 loss: 1.656411 \tcontrastive loss: 1.471881\n",
      "Epoch: 392 \tTrain Loss: 5.764515 \tmse loss: 2.255524 \tmse2 loss: 1.655149 \tcontrastive loss: 1.853841\n",
      "Epoch: 393 \tTrain Loss: 5.297834 \tmse loss: 2.266213 \tmse2 loss: 1.683914 \tcontrastive loss: 1.347706\n",
      "Epoch: 394 \tTrain Loss: 4.809654 \tmse loss: 2.257812 \tmse2 loss: 1.566163 \tcontrastive loss: 0.985679\n",
      "Saving..\n",
      "Epoch: 395 \tTrain Loss: 5.592052 \tmse loss: 2.234019 \tmse2 loss: 1.627429 \tcontrastive loss: 1.730604\n",
      "Epoch: 396 \tTrain Loss: 7.010032 \tmse loss: 2.307899 \tmse2 loss: 1.665420 \tcontrastive loss: 3.036713\n",
      "Epoch: 397 \tTrain Loss: 9.023223 \tmse loss: 2.383066 \tmse2 loss: 1.811641 \tcontrastive loss: 4.828516\n",
      "Epoch: 398 \tTrain Loss: 5.977553 \tmse loss: 2.325400 \tmse2 loss: 1.599087 \tcontrastive loss: 2.053066\n",
      "Epoch: 399 \tTrain Loss: 5.438116 \tmse loss: 2.316053 \tmse2 loss: 1.592087 \tcontrastive loss: 1.529975\n",
      "Epoch: 400 \tTrain Loss: 6.006997 \tmse loss: 2.285563 \tmse2 loss: 1.689592 \tcontrastive loss: 2.031843\n",
      "Epoch: 401 \tTrain Loss: 5.804307 \tmse loss: 2.219332 \tmse2 loss: 1.626825 \tcontrastive loss: 1.958149\n",
      "Epoch: 402 \tTrain Loss: 5.775519 \tmse loss: 2.277311 \tmse2 loss: 1.622487 \tcontrastive loss: 1.875721\n",
      "Epoch: 403 \tTrain Loss: 5.494017 \tmse loss: 2.312160 \tmse2 loss: 1.569034 \tcontrastive loss: 1.612824\n",
      "Epoch: 404 \tTrain Loss: 6.125317 \tmse loss: 2.319088 \tmse2 loss: 1.644423 \tcontrastive loss: 2.161807\n",
      "Epoch: 405 \tTrain Loss: 5.800868 \tmse loss: 2.293864 \tmse2 loss: 1.654863 \tcontrastive loss: 1.852141\n",
      "Epoch: 406 \tTrain Loss: 6.893444 \tmse loss: 2.319046 \tmse2 loss: 1.728307 \tcontrastive loss: 2.846090\n",
      "Epoch: 407 \tTrain Loss: 5.426156 \tmse loss: 2.279917 \tmse2 loss: 1.737212 \tcontrastive loss: 1.409027\n",
      "Epoch: 408 \tTrain Loss: 5.971263 \tmse loss: 2.250141 \tmse2 loss: 1.633385 \tcontrastive loss: 2.087737\n",
      "Epoch: 409 \tTrain Loss: 5.517773 \tmse loss: 2.341008 \tmse2 loss: 1.692968 \tcontrastive loss: 1.483796\n",
      "Epoch: 410 \tTrain Loss: 5.652699 \tmse loss: 2.277719 \tmse2 loss: 1.600230 \tcontrastive loss: 1.774750\n",
      "Epoch: 411 \tTrain Loss: 4.940246 \tmse loss: 2.251069 \tmse2 loss: 1.605591 \tcontrastive loss: 1.083586\n",
      "Epoch: 412 \tTrain Loss: 6.074018 \tmse loss: 2.244941 \tmse2 loss: 1.678864 \tcontrastive loss: 2.150213\n",
      "Epoch: 413 \tTrain Loss: 5.450743 \tmse loss: 2.300781 \tmse2 loss: 1.649207 \tcontrastive loss: 1.500756\n",
      "Epoch: 414 \tTrain Loss: 6.501922 \tmse loss: 2.265454 \tmse2 loss: 1.759546 \tcontrastive loss: 2.476921\n",
      "Epoch: 415 \tTrain Loss: 5.885758 \tmse loss: 2.256056 \tmse2 loss: 1.713251 \tcontrastive loss: 1.916451\n",
      "Epoch: 416 \tTrain Loss: 5.069332 \tmse loss: 2.245672 \tmse2 loss: 1.624431 \tcontrastive loss: 1.199230\n",
      "Epoch: 417 \tTrain Loss: 5.177335 \tmse loss: 2.252448 \tmse2 loss: 1.616196 \tcontrastive loss: 1.308691\n",
      "Epoch: 418 \tTrain Loss: 6.310687 \tmse loss: 2.280738 \tmse2 loss: 1.669336 \tcontrastive loss: 2.360613\n",
      "Epoch: 419 \tTrain Loss: 6.076380 \tmse loss: 2.271735 \tmse2 loss: 1.675460 \tcontrastive loss: 2.129186\n",
      "Epoch: 420 \tTrain Loss: 5.957319 \tmse loss: 2.235412 \tmse2 loss: 1.552001 \tcontrastive loss: 2.169905\n",
      "Epoch: 421 \tTrain Loss: 6.127647 \tmse loss: 2.243935 \tmse2 loss: 1.772953 \tcontrastive loss: 2.110760\n",
      "Epoch: 422 \tTrain Loss: 6.147655 \tmse loss: 2.218503 \tmse2 loss: 1.554474 \tcontrastive loss: 2.374677\n",
      "Epoch: 423 \tTrain Loss: 6.076436 \tmse loss: 2.259806 \tmse2 loss: 1.610878 \tcontrastive loss: 2.205752\n",
      "Epoch: 424 \tTrain Loss: 6.018474 \tmse loss: 2.236378 \tmse2 loss: 1.599989 \tcontrastive loss: 2.182107\n",
      "Epoch: 425 \tTrain Loss: 6.012299 \tmse loss: 2.239433 \tmse2 loss: 1.673840 \tcontrastive loss: 2.099025\n",
      "Epoch: 426 \tTrain Loss: 5.056585 \tmse loss: 2.208675 \tmse2 loss: 1.607831 \tcontrastive loss: 1.240079\n",
      "Epoch: 427 \tTrain Loss: 4.653129 \tmse loss: 2.166119 \tmse2 loss: 1.718594 \tcontrastive loss: 0.768415\n",
      "Saving..\n",
      "Epoch: 428 \tTrain Loss: 5.496115 \tmse loss: 2.187814 \tmse2 loss: 1.495677 \tcontrastive loss: 1.812624\n",
      "Epoch: 429 \tTrain Loss: 5.933039 \tmse loss: 2.193334 \tmse2 loss: 1.611718 \tcontrastive loss: 2.127987\n",
      "Epoch: 430 \tTrain Loss: 5.326921 \tmse loss: 2.211725 \tmse2 loss: 1.579927 \tcontrastive loss: 1.535269\n",
      "Epoch: 431 \tTrain Loss: 5.535938 \tmse loss: 2.197191 \tmse2 loss: 1.567471 \tcontrastive loss: 1.771276\n",
      "Epoch: 432 \tTrain Loss: 5.106696 \tmse loss: 2.166506 \tmse2 loss: 1.546913 \tcontrastive loss: 1.393277\n",
      "Epoch: 433 \tTrain Loss: 5.144530 \tmse loss: 2.232906 \tmse2 loss: 1.660664 \tcontrastive loss: 1.250959\n",
      "Epoch: 434 \tTrain Loss: 6.171270 \tmse loss: 2.228134 \tmse2 loss: 1.603136 \tcontrastive loss: 2.340000\n",
      "Epoch: 435 \tTrain Loss: 5.909020 \tmse loss: 2.206304 \tmse2 loss: 1.621835 \tcontrastive loss: 2.080881\n",
      "Epoch: 436 \tTrain Loss: 5.154165 \tmse loss: 2.207765 \tmse2 loss: 1.645232 \tcontrastive loss: 1.301168\n",
      "Epoch: 437 \tTrain Loss: 5.084822 \tmse loss: 2.118320 \tmse2 loss: 1.546129 \tcontrastive loss: 1.420372\n",
      "Epoch: 438 \tTrain Loss: 5.157563 \tmse loss: 2.125928 \tmse2 loss: 1.523588 \tcontrastive loss: 1.508047\n",
      "Epoch: 439 \tTrain Loss: 5.231825 \tmse loss: 2.136955 \tmse2 loss: 1.546478 \tcontrastive loss: 1.548392\n",
      "Epoch: 440 \tTrain Loss: 6.756743 \tmse loss: 2.209221 \tmse2 loss: 1.648628 \tcontrastive loss: 2.898895\n",
      "Epoch: 441 \tTrain Loss: 5.847412 \tmse loss: 2.179595 \tmse2 loss: 1.499612 \tcontrastive loss: 2.168204\n",
      "Epoch: 442 \tTrain Loss: 4.444048 \tmse loss: 2.102451 \tmse2 loss: 1.507463 \tcontrastive loss: 0.834135\n",
      "Saving..\n",
      "Epoch: 443 \tTrain Loss: 4.712718 \tmse loss: 2.143600 \tmse2 loss: 1.511297 \tcontrastive loss: 1.057821\n",
      "Epoch: 444 \tTrain Loss: 5.307805 \tmse loss: 2.118321 \tmse2 loss: 1.527197 \tcontrastive loss: 1.662287\n",
      "Epoch: 445 \tTrain Loss: 6.059366 \tmse loss: 2.204820 \tmse2 loss: 1.717226 \tcontrastive loss: 2.137320\n",
      "Epoch: 446 \tTrain Loss: 5.845703 \tmse loss: 2.225245 \tmse2 loss: 1.708991 \tcontrastive loss: 1.911466\n",
      "Epoch: 447 \tTrain Loss: 5.582422 \tmse loss: 2.211843 \tmse2 loss: 1.718394 \tcontrastive loss: 1.652185\n",
      "Epoch: 448 \tTrain Loss: 5.596721 \tmse loss: 2.167831 \tmse2 loss: 1.530395 \tcontrastive loss: 1.898494\n",
      "Epoch: 449 \tTrain Loss: 5.521288 \tmse loss: 2.169474 \tmse2 loss: 1.575868 \tcontrastive loss: 1.775945\n",
      "Epoch: 450 \tTrain Loss: 6.086628 \tmse loss: 2.189772 \tmse2 loss: 1.724191 \tcontrastive loss: 2.172664\n",
      "Epoch: 451 \tTrain Loss: 5.627651 \tmse loss: 2.189675 \tmse2 loss: 1.496909 \tcontrastive loss: 1.941067\n",
      "Epoch: 452 \tTrain Loss: 5.939195 \tmse loss: 2.175949 \tmse2 loss: 1.612478 \tcontrastive loss: 2.150769\n",
      "Epoch: 453 \tTrain Loss: 4.332236 \tmse loss: 2.144383 \tmse2 loss: 1.594887 \tcontrastive loss: 0.592966\n",
      "Saving..\n",
      "Epoch: 454 \tTrain Loss: 5.153403 \tmse loss: 2.177244 \tmse2 loss: 1.483266 \tcontrastive loss: 1.492893\n",
      "Epoch: 455 \tTrain Loss: 5.005316 \tmse loss: 2.153013 \tmse2 loss: 1.586865 \tcontrastive loss: 1.265438\n",
      "Epoch: 456 \tTrain Loss: 4.834699 \tmse loss: 2.148894 \tmse2 loss: 1.546765 \tcontrastive loss: 1.139040\n",
      "Epoch: 457 \tTrain Loss: 5.614229 \tmse loss: 2.173138 \tmse2 loss: 1.590430 \tcontrastive loss: 1.850662\n",
      "Epoch: 458 \tTrain Loss: 6.054192 \tmse loss: 2.117488 \tmse2 loss: 1.544380 \tcontrastive loss: 2.392325\n",
      "Epoch: 459 \tTrain Loss: 6.057418 \tmse loss: 2.131742 \tmse2 loss: 1.510504 \tcontrastive loss: 2.415172\n",
      "Epoch: 460 \tTrain Loss: 5.547174 \tmse loss: 2.152308 \tmse2 loss: 1.529129 \tcontrastive loss: 1.865737\n",
      "Epoch: 461 \tTrain Loss: 4.804241 \tmse loss: 2.122882 \tmse2 loss: 1.599296 \tcontrastive loss: 1.082063\n",
      "Epoch: 462 \tTrain Loss: 4.670696 \tmse loss: 2.106455 \tmse2 loss: 1.582118 \tcontrastive loss: 0.982123\n",
      "Epoch: 463 \tTrain Loss: 4.561766 \tmse loss: 2.110534 \tmse2 loss: 1.513020 \tcontrastive loss: 0.938212\n",
      "Epoch: 464 \tTrain Loss: 4.396139 \tmse loss: 2.085576 \tmse2 loss: 1.492900 \tcontrastive loss: 0.817663\n",
      "Epoch: 465 \tTrain Loss: 4.538793 \tmse loss: 2.120267 \tmse2 loss: 1.474819 \tcontrastive loss: 0.943707\n",
      "Epoch: 466 \tTrain Loss: 4.589502 \tmse loss: 2.067170 \tmse2 loss: 1.404822 \tcontrastive loss: 1.117510\n",
      "Epoch: 467 \tTrain Loss: 5.403352 \tmse loss: 2.112116 \tmse2 loss: 1.680000 \tcontrastive loss: 1.611236\n",
      "Epoch: 468 \tTrain Loss: 4.897285 \tmse loss: 2.070948 \tmse2 loss: 1.529014 \tcontrastive loss: 1.297324\n",
      "Epoch: 469 \tTrain Loss: 6.171736 \tmse loss: 2.076703 \tmse2 loss: 1.521036 \tcontrastive loss: 2.573997\n",
      "Epoch: 470 \tTrain Loss: 5.466201 \tmse loss: 2.129908 \tmse2 loss: 1.688749 \tcontrastive loss: 1.647544\n",
      "Epoch: 471 \tTrain Loss: 5.215383 \tmse loss: 2.072433 \tmse2 loss: 1.501229 \tcontrastive loss: 1.641721\n",
      "Epoch: 472 \tTrain Loss: 5.355619 \tmse loss: 2.087898 \tmse2 loss: 1.570626 \tcontrastive loss: 1.697095\n",
      "Epoch: 473 \tTrain Loss: 6.609835 \tmse loss: 2.118280 \tmse2 loss: 1.496672 \tcontrastive loss: 2.994884\n",
      "Epoch: 474 \tTrain Loss: 5.035813 \tmse loss: 2.156842 \tmse2 loss: 1.476814 \tcontrastive loss: 1.402157\n",
      "Epoch: 475 \tTrain Loss: 6.519207 \tmse loss: 2.173410 \tmse2 loss: 1.597325 \tcontrastive loss: 2.748472\n",
      "Epoch: 476 \tTrain Loss: 6.311858 \tmse loss: 2.156212 \tmse2 loss: 1.565324 \tcontrastive loss: 2.590322\n",
      "Epoch: 477 \tTrain Loss: 5.666112 \tmse loss: 2.074645 \tmse2 loss: 1.525146 \tcontrastive loss: 2.066321\n",
      "Epoch: 478 \tTrain Loss: 5.488390 \tmse loss: 2.113490 \tmse2 loss: 1.630230 \tcontrastive loss: 1.744670\n",
      "Epoch: 479 \tTrain Loss: 4.777593 \tmse loss: 2.065197 \tmse2 loss: 1.453740 \tcontrastive loss: 1.258657\n",
      "Epoch: 480 \tTrain Loss: 5.459015 \tmse loss: 2.065495 \tmse2 loss: 1.552274 \tcontrastive loss: 1.841246\n",
      "Epoch: 481 \tTrain Loss: 5.390632 \tmse loss: 2.048453 \tmse2 loss: 1.462661 \tcontrastive loss: 1.879517\n",
      "Epoch: 482 \tTrain Loss: 6.414175 \tmse loss: 2.134704 \tmse2 loss: 1.606392 \tcontrastive loss: 2.673079\n",
      "Epoch: 483 \tTrain Loss: 5.548980 \tmse loss: 2.102865 \tmse2 loss: 1.546319 \tcontrastive loss: 1.899796\n",
      "Epoch: 484 \tTrain Loss: 4.706725 \tmse loss: 2.140876 \tmse2 loss: 1.462725 \tcontrastive loss: 1.103123\n",
      "Epoch: 485 \tTrain Loss: 4.567474 \tmse loss: 2.068552 \tmse2 loss: 1.409506 \tcontrastive loss: 1.089415\n",
      "Epoch: 486 \tTrain Loss: 4.524792 \tmse loss: 2.062871 \tmse2 loss: 1.450259 \tcontrastive loss: 1.011662\n",
      "Epoch: 487 \tTrain Loss: 4.377731 \tmse loss: 2.023632 \tmse2 loss: 1.461930 \tcontrastive loss: 0.892169\n",
      "Epoch: 488 \tTrain Loss: 4.719852 \tmse loss: 2.108426 \tmse2 loss: 1.468866 \tcontrastive loss: 1.142559\n",
      "Epoch: 489 \tTrain Loss: 4.209064 \tmse loss: 2.044673 \tmse2 loss: 1.558449 \tcontrastive loss: 0.605941\n",
      "Saving..\n",
      "Epoch: 490 \tTrain Loss: 4.183729 \tmse loss: 2.011999 \tmse2 loss: 1.554312 \tcontrastive loss: 0.617418\n",
      "Saving..\n",
      "Epoch: 491 \tTrain Loss: 5.090063 \tmse loss: 1.975822 \tmse2 loss: 1.438975 \tcontrastive loss: 1.675266\n",
      "Epoch: 492 \tTrain Loss: 5.677122 \tmse loss: 2.010517 \tmse2 loss: 1.515657 \tcontrastive loss: 2.150948\n",
      "Epoch: 493 \tTrain Loss: 6.667644 \tmse loss: 2.035389 \tmse2 loss: 1.404622 \tcontrastive loss: 3.227633\n",
      "Epoch: 494 \tTrain Loss: 5.763667 \tmse loss: 2.108472 \tmse2 loss: 1.516442 \tcontrastive loss: 2.138754\n",
      "Epoch: 495 \tTrain Loss: 5.243407 \tmse loss: 2.119835 \tmse2 loss: 1.423227 \tcontrastive loss: 1.700346\n",
      "Epoch: 496 \tTrain Loss: 4.643746 \tmse loss: 2.033792 \tmse2 loss: 1.364118 \tcontrastive loss: 1.245836\n",
      "Epoch: 497 \tTrain Loss: 4.619389 \tmse loss: 2.014785 \tmse2 loss: 1.436883 \tcontrastive loss: 1.167720\n",
      "Epoch: 498 \tTrain Loss: 4.872244 \tmse loss: 2.068534 \tmse2 loss: 1.500503 \tcontrastive loss: 1.303208\n",
      "Epoch: 499 \tTrain Loss: 5.916058 \tmse loss: 2.060641 \tmse2 loss: 1.468148 \tcontrastive loss: 2.387269\n",
      "Epoch: 500 \tTrain Loss: 4.801714 \tmse loss: 1.960291 \tmse2 loss: 1.504062 \tcontrastive loss: 1.337361\n",
      "Epoch: 501 \tTrain Loss: 5.379452 \tmse loss: 1.984928 \tmse2 loss: 1.404044 \tcontrastive loss: 1.990480\n",
      "Epoch: 502 \tTrain Loss: 5.827217 \tmse loss: 2.026107 \tmse2 loss: 1.440768 \tcontrastive loss: 2.360343\n",
      "Epoch: 503 \tTrain Loss: 5.266105 \tmse loss: 2.004221 \tmse2 loss: 1.496506 \tcontrastive loss: 1.765378\n",
      "Epoch: 504 \tTrain Loss: 4.551402 \tmse loss: 2.007078 \tmse2 loss: 1.389502 \tcontrastive loss: 1.154822\n",
      "Epoch: 505 \tTrain Loss: 5.045224 \tmse loss: 2.086195 \tmse2 loss: 1.554783 \tcontrastive loss: 1.404245\n",
      "Epoch: 506 \tTrain Loss: 4.327721 \tmse loss: 2.027959 \tmse2 loss: 1.497664 \tcontrastive loss: 0.802098\n",
      "Epoch: 507 \tTrain Loss: 4.048852 \tmse loss: 1.961885 \tmse2 loss: 1.389223 \tcontrastive loss: 0.697744\n",
      "Saving..\n",
      "Epoch: 508 \tTrain Loss: 4.524564 \tmse loss: 1.936563 \tmse2 loss: 1.310492 \tcontrastive loss: 1.277509\n",
      "Epoch: 509 \tTrain Loss: 4.431625 \tmse loss: 1.946392 \tmse2 loss: 1.420968 \tcontrastive loss: 1.064264\n",
      "Epoch: 510 \tTrain Loss: 4.788553 \tmse loss: 1.950428 \tmse2 loss: 1.424896 \tcontrastive loss: 1.413230\n",
      "Epoch: 511 \tTrain Loss: 4.536377 \tmse loss: 1.988811 \tmse2 loss: 1.337279 \tcontrastive loss: 1.210287\n",
      "Epoch: 512 \tTrain Loss: 5.441385 \tmse loss: 2.009631 \tmse2 loss: 1.498762 \tcontrastive loss: 1.932991\n",
      "Epoch: 513 \tTrain Loss: 5.598900 \tmse loss: 1.979083 \tmse2 loss: 1.500570 \tcontrastive loss: 2.119247\n",
      "Epoch: 514 \tTrain Loss: 5.760483 \tmse loss: 2.027096 \tmse2 loss: 1.488637 \tcontrastive loss: 2.244751\n",
      "Epoch: 515 \tTrain Loss: 5.649076 \tmse loss: 2.020833 \tmse2 loss: 1.443873 \tcontrastive loss: 2.184370\n",
      "Epoch: 516 \tTrain Loss: 5.519977 \tmse loss: 2.050030 \tmse2 loss: 1.372923 \tcontrastive loss: 2.097024\n",
      "Epoch: 517 \tTrain Loss: 5.207576 \tmse loss: 2.084217 \tmse2 loss: 1.457533 \tcontrastive loss: 1.665826\n",
      "Epoch: 518 \tTrain Loss: 4.297697 \tmse loss: 1.994202 \tmse2 loss: 1.425486 \tcontrastive loss: 0.878008\n",
      "Epoch: 519 \tTrain Loss: 4.950482 \tmse loss: 1.949536 \tmse2 loss: 1.422872 \tcontrastive loss: 1.578074\n",
      "Epoch: 520 \tTrain Loss: 5.150687 \tmse loss: 2.012716 \tmse2 loss: 1.512711 \tcontrastive loss: 1.625259\n",
      "Epoch: 521 \tTrain Loss: 4.264164 \tmse loss: 1.933947 \tmse2 loss: 1.439274 \tcontrastive loss: 0.890943\n",
      "Epoch: 522 \tTrain Loss: 3.894712 \tmse loss: 1.925559 \tmse2 loss: 1.401848 \tcontrastive loss: 0.567305\n",
      "Saving..\n",
      "Epoch: 523 \tTrain Loss: 4.330287 \tmse loss: 1.926846 \tmse2 loss: 1.366022 \tcontrastive loss: 1.037419\n",
      "Epoch: 524 \tTrain Loss: 4.856059 \tmse loss: 1.932145 \tmse2 loss: 1.429863 \tcontrastive loss: 1.494051\n",
      "Epoch: 525 \tTrain Loss: 4.904092 \tmse loss: 1.950118 \tmse2 loss: 1.406522 \tcontrastive loss: 1.547453\n",
      "Epoch: 526 \tTrain Loss: 4.263606 \tmse loss: 1.999680 \tmse2 loss: 1.410679 \tcontrastive loss: 0.853247\n",
      "Epoch: 527 \tTrain Loss: 5.211519 \tmse loss: 1.938709 \tmse2 loss: 1.374941 \tcontrastive loss: 1.897869\n",
      "Epoch: 528 \tTrain Loss: 5.007451 \tmse loss: 2.051989 \tmse2 loss: 1.500019 \tcontrastive loss: 1.455444\n",
      "Epoch: 529 \tTrain Loss: 5.898875 \tmse loss: 2.006547 \tmse2 loss: 1.422741 \tcontrastive loss: 2.469587\n",
      "Epoch: 530 \tTrain Loss: 6.033808 \tmse loss: 2.016005 \tmse2 loss: 1.421474 \tcontrastive loss: 2.596328\n",
      "Epoch: 531 \tTrain Loss: 5.341341 \tmse loss: 2.016269 \tmse2 loss: 1.531012 \tcontrastive loss: 1.794059\n",
      "Epoch: 532 \tTrain Loss: 5.483036 \tmse loss: 2.031417 \tmse2 loss: 1.541777 \tcontrastive loss: 1.909842\n",
      "Epoch: 533 \tTrain Loss: 5.353294 \tmse loss: 2.001598 \tmse2 loss: 1.542740 \tcontrastive loss: 1.808956\n",
      "Epoch: 534 \tTrain Loss: 4.025332 \tmse loss: 1.978674 \tmse2 loss: 1.383244 \tcontrastive loss: 0.663415\n",
      "Epoch: 535 \tTrain Loss: 4.101821 \tmse loss: 1.960397 \tmse2 loss: 1.445650 \tcontrastive loss: 0.695774\n",
      "Epoch: 536 \tTrain Loss: 4.233617 \tmse loss: 1.932154 \tmse2 loss: 1.395087 \tcontrastive loss: 0.906376\n",
      "Epoch: 537 \tTrain Loss: 5.180076 \tmse loss: 1.949458 \tmse2 loss: 1.443943 \tcontrastive loss: 1.786675\n",
      "Epoch: 538 \tTrain Loss: 5.536276 \tmse loss: 1.967499 \tmse2 loss: 1.431542 \tcontrastive loss: 2.137235\n",
      "Epoch: 539 \tTrain Loss: 5.357451 \tmse loss: 2.036911 \tmse2 loss: 1.356731 \tcontrastive loss: 1.963809\n",
      "Epoch: 540 \tTrain Loss: 4.376557 \tmse loss: 1.946776 \tmse2 loss: 1.356342 \tcontrastive loss: 1.073439\n",
      "Epoch: 541 \tTrain Loss: 4.224624 \tmse loss: 1.880984 \tmse2 loss: 1.407660 \tcontrastive loss: 0.935980\n",
      "Epoch: 542 \tTrain Loss: 4.071344 \tmse loss: 1.891915 \tmse2 loss: 1.343189 \tcontrastive loss: 0.836240\n",
      "Epoch: 543 \tTrain Loss: 4.988475 \tmse loss: 1.915134 \tmse2 loss: 1.445459 \tcontrastive loss: 1.627882\n",
      "Epoch: 544 \tTrain Loss: 4.780155 \tmse loss: 1.986278 \tmse2 loss: 1.512846 \tcontrastive loss: 1.281032\n",
      "Epoch: 545 \tTrain Loss: 4.794338 \tmse loss: 1.946985 \tmse2 loss: 1.432939 \tcontrastive loss: 1.414414\n",
      "Epoch: 546 \tTrain Loss: 4.530675 \tmse loss: 1.911675 \tmse2 loss: 1.337296 \tcontrastive loss: 1.281704\n",
      "Epoch: 547 \tTrain Loss: 5.282916 \tmse loss: 1.928342 \tmse2 loss: 1.469161 \tcontrastive loss: 1.885413\n",
      "Epoch: 548 \tTrain Loss: 4.908722 \tmse loss: 1.995651 \tmse2 loss: 1.402571 \tcontrastive loss: 1.510500\n",
      "Epoch: 549 \tTrain Loss: 5.292309 \tmse loss: 1.969521 \tmse2 loss: 1.466362 \tcontrastive loss: 1.856427\n",
      "Epoch: 550 \tTrain Loss: 5.584557 \tmse loss: 1.921110 \tmse2 loss: 1.486576 \tcontrastive loss: 2.176871\n",
      "Epoch: 551 \tTrain Loss: 5.240980 \tmse loss: 1.917191 \tmse2 loss: 1.379326 \tcontrastive loss: 1.944463\n",
      "Epoch: 552 \tTrain Loss: 4.849191 \tmse loss: 1.904670 \tmse2 loss: 1.464485 \tcontrastive loss: 1.480036\n",
      "Epoch: 553 \tTrain Loss: 4.852335 \tmse loss: 1.975912 \tmse2 loss: 1.433466 \tcontrastive loss: 1.442957\n",
      "Epoch: 554 \tTrain Loss: 4.531282 \tmse loss: 1.936661 \tmse2 loss: 1.413372 \tcontrastive loss: 1.181249\n",
      "Epoch: 555 \tTrain Loss: 5.013540 \tmse loss: 1.915990 \tmse2 loss: 1.468003 \tcontrastive loss: 1.629548\n",
      "Epoch: 556 \tTrain Loss: 4.160650 \tmse loss: 1.873832 \tmse2 loss: 1.405839 \tcontrastive loss: 0.880978\n",
      "Epoch: 557 \tTrain Loss: 4.862793 \tmse loss: 1.882110 \tmse2 loss: 1.373618 \tcontrastive loss: 1.607065\n",
      "Epoch: 558 \tTrain Loss: 4.874474 \tmse loss: 1.910858 \tmse2 loss: 1.249219 \tcontrastive loss: 1.714397\n",
      "Epoch: 559 \tTrain Loss: 4.659984 \tmse loss: 1.885696 \tmse2 loss: 1.412687 \tcontrastive loss: 1.361600\n",
      "Epoch: 560 \tTrain Loss: 4.637204 \tmse loss: 1.870537 \tmse2 loss: 1.403277 \tcontrastive loss: 1.363390\n",
      "Epoch: 561 \tTrain Loss: 5.323741 \tmse loss: 1.882950 \tmse2 loss: 1.341481 \tcontrastive loss: 2.099310\n",
      "Epoch: 562 \tTrain Loss: 4.929621 \tmse loss: 1.913036 \tmse2 loss: 1.479953 \tcontrastive loss: 1.536632\n",
      "Epoch: 563 \tTrain Loss: 4.796277 \tmse loss: 1.898844 \tmse2 loss: 1.371968 \tcontrastive loss: 1.525465\n",
      "Epoch: 564 \tTrain Loss: 3.788342 \tmse loss: 1.886581 \tmse2 loss: 1.365702 \tcontrastive loss: 0.536059\n",
      "Saving..\n",
      "Epoch: 565 \tTrain Loss: 4.001258 \tmse loss: 1.880028 \tmse2 loss: 1.370560 \tcontrastive loss: 0.750670\n",
      "Epoch: 566 \tTrain Loss: 4.276577 \tmse loss: 1.937525 \tmse2 loss: 1.441035 \tcontrastive loss: 0.898017\n",
      "Epoch: 567 \tTrain Loss: 4.166729 \tmse loss: 1.862768 \tmse2 loss: 1.367507 \tcontrastive loss: 0.936454\n",
      "Epoch: 568 \tTrain Loss: 4.511116 \tmse loss: 1.877592 \tmse2 loss: 1.345096 \tcontrastive loss: 1.288429\n",
      "Epoch: 569 \tTrain Loss: 4.911838 \tmse loss: 1.853765 \tmse2 loss: 1.366753 \tcontrastive loss: 1.691320\n",
      "Epoch: 570 \tTrain Loss: 4.964870 \tmse loss: 1.895598 \tmse2 loss: 1.404804 \tcontrastive loss: 1.664468\n",
      "Epoch: 571 \tTrain Loss: 4.845340 \tmse loss: 1.867406 \tmse2 loss: 1.282855 \tcontrastive loss: 1.695080\n",
      "Epoch: 572 \tTrain Loss: 4.200200 \tmse loss: 1.829666 \tmse2 loss: 1.369511 \tcontrastive loss: 1.001023\n",
      "Epoch: 573 \tTrain Loss: 4.528880 \tmse loss: 1.853944 \tmse2 loss: 1.511876 \tcontrastive loss: 1.163060\n",
      "Epoch: 574 \tTrain Loss: 4.055545 \tmse loss: 1.871414 \tmse2 loss: 1.372537 \tcontrastive loss: 0.811594\n",
      "Epoch: 575 \tTrain Loss: 4.774774 \tmse loss: 1.838014 \tmse2 loss: 1.432639 \tcontrastive loss: 1.504121\n",
      "Epoch: 576 \tTrain Loss: 3.835699 \tmse loss: 1.797290 \tmse2 loss: 1.296219 \tcontrastive loss: 0.742191\n",
      "Epoch: 577 \tTrain Loss: 4.405137 \tmse loss: 1.807577 \tmse2 loss: 1.238759 \tcontrastive loss: 1.358800\n",
      "Epoch: 578 \tTrain Loss: 4.978855 \tmse loss: 1.864351 \tmse2 loss: 1.319276 \tcontrastive loss: 1.795228\n",
      "Epoch: 579 \tTrain Loss: 4.464320 \tmse loss: 1.860971 \tmse2 loss: 1.268465 \tcontrastive loss: 1.334884\n",
      "Epoch: 580 \tTrain Loss: 5.533680 \tmse loss: 1.909540 \tmse2 loss: 1.282250 \tcontrastive loss: 2.341890\n",
      "Epoch: 581 \tTrain Loss: 4.978620 \tmse loss: 1.887674 \tmse2 loss: 1.321111 \tcontrastive loss: 1.769835\n",
      "Epoch: 582 \tTrain Loss: 5.233270 \tmse loss: 1.913516 \tmse2 loss: 1.323971 \tcontrastive loss: 1.995782\n",
      "Epoch: 583 \tTrain Loss: 4.681555 \tmse loss: 1.879435 \tmse2 loss: 1.399524 \tcontrastive loss: 1.402596\n",
      "Epoch: 584 \tTrain Loss: 4.874524 \tmse loss: 1.885872 \tmse2 loss: 1.395157 \tcontrastive loss: 1.593495\n",
      "Epoch: 585 \tTrain Loss: 4.507053 \tmse loss: 1.850130 \tmse2 loss: 1.293017 \tcontrastive loss: 1.363906\n",
      "Epoch: 586 \tTrain Loss: 4.204949 \tmse loss: 1.880540 \tmse2 loss: 1.379756 \tcontrastive loss: 0.944654\n",
      "Epoch: 587 \tTrain Loss: 4.607372 \tmse loss: 1.852324 \tmse2 loss: 1.378241 \tcontrastive loss: 1.376807\n",
      "Epoch: 588 \tTrain Loss: 4.367017 \tmse loss: 1.838222 \tmse2 loss: 1.288194 \tcontrastive loss: 1.240601\n",
      "Epoch: 589 \tTrain Loss: 6.107717 \tmse loss: 1.908031 \tmse2 loss: 1.325029 \tcontrastive loss: 2.874656\n",
      "Epoch: 590 \tTrain Loss: 5.143402 \tmse loss: 1.835174 \tmse2 loss: 1.352238 \tcontrastive loss: 1.955989\n",
      "Epoch: 591 \tTrain Loss: 4.722163 \tmse loss: 1.840340 \tmse2 loss: 1.402635 \tcontrastive loss: 1.479187\n",
      "Epoch: 592 \tTrain Loss: 4.406009 \tmse loss: 1.823806 \tmse2 loss: 1.254160 \tcontrastive loss: 1.328043\n",
      "Epoch: 593 \tTrain Loss: 4.975398 \tmse loss: 1.817474 \tmse2 loss: 1.305805 \tcontrastive loss: 1.852119\n",
      "Epoch: 594 \tTrain Loss: 5.022221 \tmse loss: 1.887226 \tmse2 loss: 1.492896 \tcontrastive loss: 1.642100\n",
      "Epoch: 595 \tTrain Loss: 5.033731 \tmse loss: 1.840466 \tmse2 loss: 1.433470 \tcontrastive loss: 1.759795\n",
      "Epoch: 596 \tTrain Loss: 4.451867 \tmse loss: 1.843099 \tmse2 loss: 1.284179 \tcontrastive loss: 1.324590\n",
      "Epoch: 597 \tTrain Loss: 4.036841 \tmse loss: 1.773474 \tmse2 loss: 1.305811 \tcontrastive loss: 0.957557\n",
      "Epoch: 598 \tTrain Loss: 3.504038 \tmse loss: 1.736828 \tmse2 loss: 1.144113 \tcontrastive loss: 0.623098\n",
      "Saving..\n",
      "Epoch: 599 \tTrain Loss: 3.729951 \tmse loss: 1.757806 \tmse2 loss: 1.363171 \tcontrastive loss: 0.608973\n",
      "Epoch: 600 \tTrain Loss: 4.825271 \tmse loss: 1.828030 \tmse2 loss: 1.352745 \tcontrastive loss: 1.644497\n",
      "Epoch: 601 \tTrain Loss: 4.891813 \tmse loss: 1.858581 \tmse2 loss: 1.376292 \tcontrastive loss: 1.656939\n",
      "Epoch: 602 \tTrain Loss: 4.713443 \tmse loss: 1.822458 \tmse2 loss: 1.312609 \tcontrastive loss: 1.578376\n",
      "Epoch: 603 \tTrain Loss: 3.468186 \tmse loss: 1.767119 \tmse2 loss: 1.234220 \tcontrastive loss: 0.466847\n",
      "Saving..\n",
      "Epoch: 604 \tTrain Loss: 3.966393 \tmse loss: 1.791420 \tmse2 loss: 1.346868 \tcontrastive loss: 0.828106\n",
      "Epoch: 605 \tTrain Loss: 4.326671 \tmse loss: 1.784971 \tmse2 loss: 1.322850 \tcontrastive loss: 1.218851\n",
      "Epoch: 606 \tTrain Loss: 5.011613 \tmse loss: 1.811814 \tmse2 loss: 1.321157 \tcontrastive loss: 1.878642\n",
      "Epoch: 607 \tTrain Loss: 4.132819 \tmse loss: 1.782525 \tmse2 loss: 1.291449 \tcontrastive loss: 1.058845\n",
      "Epoch: 608 \tTrain Loss: 3.500922 \tmse loss: 1.737030 \tmse2 loss: 1.200601 \tcontrastive loss: 0.563291\n",
      "Epoch: 609 \tTrain Loss: 3.927090 \tmse loss: 1.734015 \tmse2 loss: 1.166207 \tcontrastive loss: 1.026868\n",
      "Epoch: 610 \tTrain Loss: 4.322579 \tmse loss: 1.789617 \tmse2 loss: 1.346876 \tcontrastive loss: 1.186086\n",
      "Epoch: 611 \tTrain Loss: 4.746873 \tmse loss: 1.861194 \tmse2 loss: 1.360372 \tcontrastive loss: 1.525308\n",
      "Epoch: 612 \tTrain Loss: 5.642217 \tmse loss: 1.890286 \tmse2 loss: 1.431749 \tcontrastive loss: 2.320183\n",
      "Epoch: 613 \tTrain Loss: 3.835486 \tmse loss: 1.782571 \tmse2 loss: 1.328399 \tcontrastive loss: 0.724516\n",
      "Epoch: 614 \tTrain Loss: 3.973308 \tmse loss: 1.779790 \tmse2 loss: 1.292485 \tcontrastive loss: 0.901033\n",
      "Epoch: 615 \tTrain Loss: 5.296633 \tmse loss: 1.763797 \tmse2 loss: 1.193435 \tcontrastive loss: 2.339400\n",
      "Epoch: 616 \tTrain Loss: 4.956623 \tmse loss: 1.795761 \tmse2 loss: 1.274619 \tcontrastive loss: 1.886242\n",
      "Epoch: 617 \tTrain Loss: 3.936676 \tmse loss: 1.807783 \tmse2 loss: 1.187962 \tcontrastive loss: 0.940930\n",
      "Epoch: 618 \tTrain Loss: 3.980532 \tmse loss: 1.778855 \tmse2 loss: 1.268620 \tcontrastive loss: 0.933057\n",
      "Epoch: 619 \tTrain Loss: 3.946985 \tmse loss: 1.745873 \tmse2 loss: 1.401329 \tcontrastive loss: 0.799783\n",
      "Epoch: 620 \tTrain Loss: 3.537711 \tmse loss: 1.772402 \tmse2 loss: 1.198602 \tcontrastive loss: 0.566708\n",
      "Epoch: 621 \tTrain Loss: 4.984444 \tmse loss: 1.793124 \tmse2 loss: 1.292744 \tcontrastive loss: 1.898577\n",
      "Epoch: 622 \tTrain Loss: 5.620730 \tmse loss: 1.852223 \tmse2 loss: 1.365650 \tcontrastive loss: 2.402857\n",
      "Epoch: 623 \tTrain Loss: 4.307633 \tmse loss: 1.842716 \tmse2 loss: 1.343330 \tcontrastive loss: 1.121587\n",
      "Epoch: 624 \tTrain Loss: 3.938829 \tmse loss: 1.750006 \tmse2 loss: 1.296725 \tcontrastive loss: 0.892098\n",
      "Epoch: 625 \tTrain Loss: 4.414340 \tmse loss: 1.743539 \tmse2 loss: 1.275943 \tcontrastive loss: 1.394858\n",
      "Epoch: 626 \tTrain Loss: 4.156956 \tmse loss: 1.820147 \tmse2 loss: 1.358394 \tcontrastive loss: 0.978415\n",
      "Epoch: 627 \tTrain Loss: 4.162752 \tmse loss: 1.778305 \tmse2 loss: 1.331229 \tcontrastive loss: 1.053218\n",
      "Epoch: 628 \tTrain Loss: 4.748739 \tmse loss: 1.765111 \tmse2 loss: 1.225840 \tcontrastive loss: 1.757789\n",
      "Epoch: 629 \tTrain Loss: 4.057642 \tmse loss: 1.763636 \tmse2 loss: 1.297881 \tcontrastive loss: 0.996124\n",
      "Epoch: 630 \tTrain Loss: 4.482030 \tmse loss: 1.754561 \tmse2 loss: 1.306574 \tcontrastive loss: 1.420895\n",
      "Epoch: 631 \tTrain Loss: 4.393987 \tmse loss: 1.777343 \tmse2 loss: 1.318655 \tcontrastive loss: 1.297989\n",
      "Epoch: 632 \tTrain Loss: 4.264460 \tmse loss: 1.780252 \tmse2 loss: 1.235741 \tcontrastive loss: 1.248468\n",
      "Epoch: 633 \tTrain Loss: 4.167092 \tmse loss: 1.735740 \tmse2 loss: 1.265835 \tcontrastive loss: 1.165517\n",
      "Epoch: 634 \tTrain Loss: 4.665906 \tmse loss: 1.759927 \tmse2 loss: 1.393531 \tcontrastive loss: 1.512449\n",
      "Epoch: 635 \tTrain Loss: 3.857398 \tmse loss: 1.782763 \tmse2 loss: 1.283504 \tcontrastive loss: 0.791131\n",
      "Epoch: 636 \tTrain Loss: 3.930860 \tmse loss: 1.755081 \tmse2 loss: 1.299447 \tcontrastive loss: 0.876332\n",
      "Epoch: 637 \tTrain Loss: 4.379045 \tmse loss: 1.759748 \tmse2 loss: 1.266064 \tcontrastive loss: 1.353233\n",
      "Epoch: 638 \tTrain Loss: 4.372393 \tmse loss: 1.757873 \tmse2 loss: 1.235096 \tcontrastive loss: 1.379424\n",
      "Epoch: 639 \tTrain Loss: 3.871510 \tmse loss: 1.792491 \tmse2 loss: 1.275194 \tcontrastive loss: 0.803824\n",
      "Epoch: 640 \tTrain Loss: 3.498618 \tmse loss: 1.762152 \tmse2 loss: 1.196751 \tcontrastive loss: 0.539714\n",
      "Epoch: 641 \tTrain Loss: 3.868621 \tmse loss: 1.765412 \tmse2 loss: 1.272451 \tcontrastive loss: 0.830759\n",
      "Epoch: 642 \tTrain Loss: 4.247354 \tmse loss: 1.733313 \tmse2 loss: 1.280083 \tcontrastive loss: 1.233958\n",
      "Epoch: 643 \tTrain Loss: 4.783739 \tmse loss: 1.793697 \tmse2 loss: 1.306363 \tcontrastive loss: 1.683678\n",
      "Epoch: 644 \tTrain Loss: 4.822606 \tmse loss: 1.786324 \tmse2 loss: 1.366674 \tcontrastive loss: 1.669608\n",
      "Epoch: 645 \tTrain Loss: 3.942934 \tmse loss: 1.773100 \tmse2 loss: 1.339697 \tcontrastive loss: 0.830137\n",
      "Epoch: 646 \tTrain Loss: 4.080223 \tmse loss: 1.737836 \tmse2 loss: 1.240512 \tcontrastive loss: 1.101875\n",
      "Epoch: 647 \tTrain Loss: 4.295642 \tmse loss: 1.729345 \tmse2 loss: 1.292411 \tcontrastive loss: 1.273886\n",
      "Epoch: 648 \tTrain Loss: 4.196767 \tmse loss: 1.749682 \tmse2 loss: 1.347662 \tcontrastive loss: 1.099423\n",
      "Epoch: 649 \tTrain Loss: 3.870101 \tmse loss: 1.721646 \tmse2 loss: 1.322921 \tcontrastive loss: 0.825533\n",
      "Epoch: 650 \tTrain Loss: 3.672484 \tmse loss: 1.704060 \tmse2 loss: 1.215317 \tcontrastive loss: 0.753107\n",
      "Epoch: 651 \tTrain Loss: 4.349431 \tmse loss: 1.757480 \tmse2 loss: 1.330028 \tcontrastive loss: 1.261922\n",
      "Epoch: 652 \tTrain Loss: 3.959591 \tmse loss: 1.672887 \tmse2 loss: 1.246532 \tcontrastive loss: 1.040172\n",
      "Epoch: 653 \tTrain Loss: 4.768250 \tmse loss: 1.727015 \tmse2 loss: 1.334139 \tcontrastive loss: 1.707097\n",
      "Epoch: 654 \tTrain Loss: 3.928732 \tmse loss: 1.695238 \tmse2 loss: 1.262316 \tcontrastive loss: 0.971178\n",
      "Epoch: 655 \tTrain Loss: 3.405546 \tmse loss: 1.660390 \tmse2 loss: 1.238308 \tcontrastive loss: 0.506848\n",
      "Saving..\n",
      "Epoch: 656 \tTrain Loss: 3.895733 \tmse loss: 1.727497 \tmse2 loss: 1.328512 \tcontrastive loss: 0.839724\n",
      "Epoch: 657 \tTrain Loss: 4.299247 \tmse loss: 1.729668 \tmse2 loss: 1.272130 \tcontrastive loss: 1.297449\n",
      "Epoch: 658 \tTrain Loss: 4.691504 \tmse loss: 1.734519 \tmse2 loss: 1.293566 \tcontrastive loss: 1.663419\n",
      "Epoch: 659 \tTrain Loss: 3.914517 \tmse loss: 1.689483 \tmse2 loss: 1.231694 \tcontrastive loss: 0.993340\n",
      "Epoch: 660 \tTrain Loss: 4.662078 \tmse loss: 1.744314 \tmse2 loss: 1.327525 \tcontrastive loss: 1.590239\n",
      "Epoch: 661 \tTrain Loss: 4.063377 \tmse loss: 1.725007 \tmse2 loss: 1.259451 \tcontrastive loss: 1.078919\n",
      "Epoch: 662 \tTrain Loss: 4.081391 \tmse loss: 1.714962 \tmse2 loss: 1.237972 \tcontrastive loss: 1.128457\n",
      "Epoch: 663 \tTrain Loss: 3.859375 \tmse loss: 1.703104 \tmse2 loss: 1.211136 \tcontrastive loss: 0.945135\n",
      "Epoch: 664 \tTrain Loss: 4.432265 \tmse loss: 1.741872 \tmse2 loss: 1.217881 \tcontrastive loss: 1.472512\n",
      "Epoch: 665 \tTrain Loss: 4.925308 \tmse loss: 1.729317 \tmse2 loss: 1.234334 \tcontrastive loss: 1.961657\n",
      "Epoch: 666 \tTrain Loss: 4.572216 \tmse loss: 1.716211 \tmse2 loss: 1.180733 \tcontrastive loss: 1.675272\n",
      "Epoch: 667 \tTrain Loss: 3.732260 \tmse loss: 1.695642 \tmse2 loss: 1.250204 \tcontrastive loss: 0.786414\n",
      "Epoch: 668 \tTrain Loss: 3.198979 \tmse loss: 1.667342 \tmse2 loss: 1.157407 \tcontrastive loss: 0.374230\n",
      "Saving..\n",
      "Epoch: 669 \tTrain Loss: 4.384333 \tmse loss: 1.670732 \tmse2 loss: 1.243525 \tcontrastive loss: 1.470076\n",
      "Epoch: 670 \tTrain Loss: 3.687231 \tmse loss: 1.703378 \tmse2 loss: 1.235806 \tcontrastive loss: 0.748047\n",
      "Epoch: 671 \tTrain Loss: 4.257847 \tmse loss: 1.666700 \tmse2 loss: 1.197470 \tcontrastive loss: 1.393677\n",
      "Epoch: 672 \tTrain Loss: 4.045137 \tmse loss: 1.704687 \tmse2 loss: 1.180482 \tcontrastive loss: 1.159968\n",
      "Epoch: 673 \tTrain Loss: 3.635143 \tmse loss: 1.700383 \tmse2 loss: 1.267121 \tcontrastive loss: 0.667639\n",
      "Epoch: 674 \tTrain Loss: 3.918163 \tmse loss: 1.680098 \tmse2 loss: 1.198642 \tcontrastive loss: 1.039423\n",
      "Epoch: 675 \tTrain Loss: 4.357289 \tmse loss: 1.748448 \tmse2 loss: 1.202814 \tcontrastive loss: 1.406027\n",
      "Epoch: 676 \tTrain Loss: 3.758757 \tmse loss: 1.742710 \tmse2 loss: 1.244299 \tcontrastive loss: 0.771748\n",
      "Epoch: 677 \tTrain Loss: 3.923544 \tmse loss: 1.724533 \tmse2 loss: 1.247299 \tcontrastive loss: 0.951712\n",
      "Epoch: 678 \tTrain Loss: 3.430006 \tmse loss: 1.658727 \tmse2 loss: 1.218747 \tcontrastive loss: 0.552533\n",
      "Epoch: 679 \tTrain Loss: 3.980159 \tmse loss: 1.674771 \tmse2 loss: 1.249467 \tcontrastive loss: 1.055921\n",
      "Epoch: 680 \tTrain Loss: 3.952292 \tmse loss: 1.714307 \tmse2 loss: 1.312414 \tcontrastive loss: 0.925571\n",
      "Epoch: 681 \tTrain Loss: 3.722758 \tmse loss: 1.716058 \tmse2 loss: 1.304291 \tcontrastive loss: 0.702409\n",
      "Epoch: 682 \tTrain Loss: 4.490463 \tmse loss: 1.733857 \tmse2 loss: 1.287068 \tcontrastive loss: 1.469537\n",
      "Epoch: 683 \tTrain Loss: 5.035914 \tmse loss: 1.815691 \tmse2 loss: 1.289677 \tcontrastive loss: 1.930545\n",
      "Epoch: 684 \tTrain Loss: 3.489215 \tmse loss: 1.704065 \tmse2 loss: 1.130712 \tcontrastive loss: 0.654438\n",
      "Epoch: 685 \tTrain Loss: 3.762994 \tmse loss: 1.644513 \tmse2 loss: 1.222032 \tcontrastive loss: 0.896448\n",
      "Epoch: 686 \tTrain Loss: 3.959442 \tmse loss: 1.658081 \tmse2 loss: 1.234480 \tcontrastive loss: 1.066882\n",
      "Epoch: 687 \tTrain Loss: 4.073184 \tmse loss: 1.701919 \tmse2 loss: 1.163155 \tcontrastive loss: 1.208111\n",
      "Epoch: 688 \tTrain Loss: 4.463004 \tmse loss: 1.738100 \tmse2 loss: 1.281939 \tcontrastive loss: 1.442965\n",
      "Epoch: 689 \tTrain Loss: 4.641119 \tmse loss: 1.695870 \tmse2 loss: 1.302833 \tcontrastive loss: 1.642416\n",
      "Epoch: 690 \tTrain Loss: 4.320964 \tmse loss: 1.703446 \tmse2 loss: 1.295411 \tcontrastive loss: 1.322106\n",
      "Epoch: 691 \tTrain Loss: 4.691170 \tmse loss: 1.698759 \tmse2 loss: 1.238978 \tcontrastive loss: 1.753432\n",
      "Epoch: 692 \tTrain Loss: 5.174892 \tmse loss: 1.803738 \tmse2 loss: 1.334297 \tcontrastive loss: 2.036857\n",
      "Epoch: 693 \tTrain Loss: 4.485266 \tmse loss: 1.726516 \tmse2 loss: 1.224094 \tcontrastive loss: 1.534657\n",
      "Epoch: 694 \tTrain Loss: 4.072128 \tmse loss: 1.708517 \tmse2 loss: 1.335589 \tcontrastive loss: 1.028022\n",
      "Epoch: 695 \tTrain Loss: 3.658214 \tmse loss: 1.664710 \tmse2 loss: 1.191241 \tcontrastive loss: 0.802264\n",
      "Epoch: 696 \tTrain Loss: 3.337576 \tmse loss: 1.630703 \tmse2 loss: 1.132639 \tcontrastive loss: 0.574234\n",
      "Epoch: 697 \tTrain Loss: 3.701744 \tmse loss: 1.647089 \tmse2 loss: 1.070355 \tcontrastive loss: 0.984300\n",
      "Epoch: 698 \tTrain Loss: 3.934349 \tmse loss: 1.642321 \tmse2 loss: 1.145359 \tcontrastive loss: 1.146670\n",
      "Epoch: 699 \tTrain Loss: 3.555601 \tmse loss: 1.679049 \tmse2 loss: 1.225382 \tcontrastive loss: 0.651171\n",
      "Epoch: 700 \tTrain Loss: 3.454897 \tmse loss: 1.663572 \tmse2 loss: 1.219417 \tcontrastive loss: 0.571908\n",
      "Epoch: 701 \tTrain Loss: 3.905138 \tmse loss: 1.634047 \tmse2 loss: 1.193387 \tcontrastive loss: 1.077704\n",
      "Epoch: 702 \tTrain Loss: 4.141230 \tmse loss: 1.610072 \tmse2 loss: 1.236508 \tcontrastive loss: 1.294651\n",
      "Epoch: 703 \tTrain Loss: 3.948713 \tmse loss: 1.653559 \tmse2 loss: 1.230025 \tcontrastive loss: 1.065129\n",
      "Epoch: 704 \tTrain Loss: 3.934465 \tmse loss: 1.640877 \tmse2 loss: 1.237109 \tcontrastive loss: 1.056480\n",
      "Epoch: 705 \tTrain Loss: 3.646017 \tmse loss: 1.618410 \tmse2 loss: 1.222722 \tcontrastive loss: 0.804884\n",
      "Epoch: 706 \tTrain Loss: 4.185757 \tmse loss: 1.613351 \tmse2 loss: 1.194382 \tcontrastive loss: 1.378024\n",
      "Epoch: 707 \tTrain Loss: 5.105084 \tmse loss: 1.634792 \tmse2 loss: 1.164976 \tcontrastive loss: 2.305316\n",
      "Epoch: 708 \tTrain Loss: 4.404964 \tmse loss: 1.669925 \tmse2 loss: 1.190865 \tcontrastive loss: 1.544174\n",
      "Epoch: 709 \tTrain Loss: 4.033136 \tmse loss: 1.678693 \tmse2 loss: 1.154229 \tcontrastive loss: 1.200214\n",
      "Epoch: 710 \tTrain Loss: 4.004004 \tmse loss: 1.643978 \tmse2 loss: 1.122439 \tcontrastive loss: 1.237587\n",
      "Epoch: 711 \tTrain Loss: 3.150591 \tmse loss: 1.651573 \tmse2 loss: 1.110261 \tcontrastive loss: 0.388756\n",
      "Saving..\n",
      "Epoch: 712 \tTrain Loss: 3.808966 \tmse loss: 1.662786 \tmse2 loss: 1.246028 \tcontrastive loss: 0.900152\n",
      "Epoch: 713 \tTrain Loss: 4.173871 \tmse loss: 1.640701 \tmse2 loss: 1.172792 \tcontrastive loss: 1.360379\n",
      "Epoch: 714 \tTrain Loss: 4.143249 \tmse loss: 1.635205 \tmse2 loss: 1.211083 \tcontrastive loss: 1.296961\n",
      "Epoch: 715 \tTrain Loss: 3.705635 \tmse loss: 1.639662 \tmse2 loss: 1.154220 \tcontrastive loss: 0.911753\n",
      "Epoch: 716 \tTrain Loss: 3.764771 \tmse loss: 1.622625 \tmse2 loss: 1.219279 \tcontrastive loss: 0.922868\n",
      "Epoch: 717 \tTrain Loss: 3.652532 \tmse loss: 1.593893 \tmse2 loss: 1.229907 \tcontrastive loss: 0.828732\n",
      "Epoch: 718 \tTrain Loss: 3.840473 \tmse loss: 1.627877 \tmse2 loss: 1.347107 \tcontrastive loss: 0.865488\n",
      "Epoch: 719 \tTrain Loss: 3.697982 \tmse loss: 1.662300 \tmse2 loss: 1.252549 \tcontrastive loss: 0.783133\n",
      "Epoch: 720 \tTrain Loss: 3.457500 \tmse loss: 1.644337 \tmse2 loss: 1.234085 \tcontrastive loss: 0.579078\n",
      "Epoch: 721 \tTrain Loss: 4.712750 \tmse loss: 1.624790 \tmse2 loss: 1.170451 \tcontrastive loss: 1.917508\n",
      "Epoch: 722 \tTrain Loss: 3.656668 \tmse loss: 1.625617 \tmse2 loss: 1.200748 \tcontrastive loss: 0.830302\n",
      "Epoch: 723 \tTrain Loss: 3.246040 \tmse loss: 1.597219 \tmse2 loss: 1.155859 \tcontrastive loss: 0.492961\n",
      "Epoch: 724 \tTrain Loss: 3.375032 \tmse loss: 1.588283 \tmse2 loss: 1.161326 \tcontrastive loss: 0.625423\n",
      "Epoch: 725 \tTrain Loss: 4.555674 \tmse loss: 1.633826 \tmse2 loss: 1.299176 \tcontrastive loss: 1.622672\n",
      "Epoch: 726 \tTrain Loss: 3.710490 \tmse loss: 1.620304 \tmse2 loss: 1.273259 \tcontrastive loss: 0.816927\n",
      "Epoch: 727 \tTrain Loss: 4.054935 \tmse loss: 1.647193 \tmse2 loss: 1.190068 \tcontrastive loss: 1.217674\n",
      "Epoch: 728 \tTrain Loss: 3.990753 \tmse loss: 1.647171 \tmse2 loss: 1.238028 \tcontrastive loss: 1.105555\n",
      "Epoch: 729 \tTrain Loss: 3.484757 \tmse loss: 1.631879 \tmse2 loss: 1.206501 \tcontrastive loss: 0.646376\n",
      "Epoch: 730 \tTrain Loss: 4.551024 \tmse loss: 1.652231 \tmse2 loss: 1.177344 \tcontrastive loss: 1.721448\n",
      "Epoch: 731 \tTrain Loss: 3.910814 \tmse loss: 1.621217 \tmse2 loss: 1.228558 \tcontrastive loss: 1.061039\n",
      "Epoch: 732 \tTrain Loss: 4.278280 \tmse loss: 1.604674 \tmse2 loss: 1.232650 \tcontrastive loss: 1.440957\n",
      "Epoch: 733 \tTrain Loss: 3.503515 \tmse loss: 1.637681 \tmse2 loss: 1.163232 \tcontrastive loss: 0.702603\n",
      "Epoch: 734 \tTrain Loss: 4.029924 \tmse loss: 1.643093 \tmse2 loss: 1.251112 \tcontrastive loss: 1.135720\n",
      "Epoch: 735 \tTrain Loss: 3.866495 \tmse loss: 1.624817 \tmse2 loss: 1.178785 \tcontrastive loss: 1.062892\n",
      "Epoch: 736 \tTrain Loss: 3.184826 \tmse loss: 1.595007 \tmse2 loss: 1.147144 \tcontrastive loss: 0.442675\n",
      "Epoch: 737 \tTrain Loss: 4.140115 \tmse loss: 1.606339 \tmse2 loss: 1.209344 \tcontrastive loss: 1.324433\n",
      "Epoch: 738 \tTrain Loss: 4.743525 \tmse loss: 1.660253 \tmse2 loss: 1.268971 \tcontrastive loss: 1.814301\n",
      "Epoch: 739 \tTrain Loss: 3.520435 \tmse loss: 1.606805 \tmse2 loss: 1.128421 \tcontrastive loss: 0.785209\n",
      "Epoch: 740 \tTrain Loss: 3.829810 \tmse loss: 1.621195 \tmse2 loss: 1.206086 \tcontrastive loss: 1.002529\n",
      "Epoch: 741 \tTrain Loss: 3.147846 \tmse loss: 1.587338 \tmse2 loss: 1.129765 \tcontrastive loss: 0.430743\n",
      "Saving..\n",
      "Epoch: 742 \tTrain Loss: 3.789136 \tmse loss: 1.605652 \tmse2 loss: 1.175177 \tcontrastive loss: 1.008308\n",
      "Epoch: 743 \tTrain Loss: 3.986901 \tmse loss: 1.595367 \tmse2 loss: 1.184261 \tcontrastive loss: 1.207273\n",
      "Epoch: 744 \tTrain Loss: 3.439256 \tmse loss: 1.575148 \tmse2 loss: 1.140293 \tcontrastive loss: 0.723815\n",
      "Epoch: 745 \tTrain Loss: 3.113078 \tmse loss: 1.534267 \tmse2 loss: 1.067883 \tcontrastive loss: 0.510928\n",
      "Saving..\n",
      "Epoch: 746 \tTrain Loss: 4.042651 \tmse loss: 1.594484 \tmse2 loss: 1.145439 \tcontrastive loss: 1.302729\n",
      "Epoch: 747 \tTrain Loss: 3.855584 \tmse loss: 1.570386 \tmse2 loss: 1.176778 \tcontrastive loss: 1.108419\n",
      "Epoch: 748 \tTrain Loss: 3.169038 \tmse loss: 1.542228 \tmse2 loss: 1.150882 \tcontrastive loss: 0.475928\n",
      "Epoch: 749 \tTrain Loss: 3.362111 \tmse loss: 1.601398 \tmse2 loss: 1.174891 \tcontrastive loss: 0.585821\n",
      "Epoch: 750 \tTrain Loss: 4.166098 \tmse loss: 1.600562 \tmse2 loss: 1.184634 \tcontrastive loss: 1.380902\n",
      "Epoch: 751 \tTrain Loss: 3.726296 \tmse loss: 1.604188 \tmse2 loss: 1.245832 \tcontrastive loss: 0.876275\n",
      "Epoch: 752 \tTrain Loss: 3.818601 \tmse loss: 1.625545 \tmse2 loss: 1.164231 \tcontrastive loss: 1.028825\n",
      "Epoch: 753 \tTrain Loss: 4.477797 \tmse loss: 1.655259 \tmse2 loss: 1.194784 \tcontrastive loss: 1.627755\n",
      "Epoch: 754 \tTrain Loss: 3.870467 \tmse loss: 1.647723 \tmse2 loss: 1.205806 \tcontrastive loss: 1.016939\n",
      "Epoch: 755 \tTrain Loss: 4.150930 \tmse loss: 1.585501 \tmse2 loss: 1.291758 \tcontrastive loss: 1.273670\n",
      "Epoch: 756 \tTrain Loss: 4.046038 \tmse loss: 1.551942 \tmse2 loss: 1.178888 \tcontrastive loss: 1.315208\n",
      "Epoch: 757 \tTrain Loss: 4.357012 \tmse loss: 1.551783 \tmse2 loss: 1.147789 \tcontrastive loss: 1.657440\n",
      "Epoch: 758 \tTrain Loss: 4.621033 \tmse loss: 1.624940 \tmse2 loss: 1.251720 \tcontrastive loss: 1.744373\n",
      "Epoch: 759 \tTrain Loss: 3.336169 \tmse loss: 1.604806 \tmse2 loss: 1.202665 \tcontrastive loss: 0.528699\n",
      "Epoch: 760 \tTrain Loss: 2.931321 \tmse loss: 1.535052 \tmse2 loss: 1.100781 \tcontrastive loss: 0.295488\n",
      "Saving..\n",
      "Epoch: 761 \tTrain Loss: 2.999764 \tmse loss: 1.517795 \tmse2 loss: 1.141121 \tcontrastive loss: 0.340848\n",
      "Epoch: 762 \tTrain Loss: 3.036735 \tmse loss: 1.554633 \tmse2 loss: 1.066630 \tcontrastive loss: 0.415472\n",
      "Epoch: 763 \tTrain Loss: 3.655846 \tmse loss: 1.541875 \tmse2 loss: 1.102334 \tcontrastive loss: 1.011637\n",
      "Epoch: 764 \tTrain Loss: 3.511035 \tmse loss: 1.556937 \tmse2 loss: 1.190682 \tcontrastive loss: 0.763415\n",
      "Epoch: 765 \tTrain Loss: 3.915855 \tmse loss: 1.584249 \tmse2 loss: 1.155399 \tcontrastive loss: 1.176208\n",
      "Epoch: 766 \tTrain Loss: 3.917243 \tmse loss: 1.607436 \tmse2 loss: 1.184605 \tcontrastive loss: 1.125202\n",
      "Epoch: 767 \tTrain Loss: 4.039172 \tmse loss: 1.595316 \tmse2 loss: 1.183098 \tcontrastive loss: 1.260757\n",
      "Epoch: 768 \tTrain Loss: 3.943415 \tmse loss: 1.596681 \tmse2 loss: 1.126360 \tcontrastive loss: 1.220374\n",
      "Epoch: 769 \tTrain Loss: 4.221145 \tmse loss: 1.590388 \tmse2 loss: 1.166081 \tcontrastive loss: 1.464677\n",
      "Epoch: 770 \tTrain Loss: 4.412651 \tmse loss: 1.581647 \tmse2 loss: 1.191391 \tcontrastive loss: 1.639613\n",
      "Epoch: 771 \tTrain Loss: 4.650579 \tmse loss: 1.604476 \tmse2 loss: 1.206185 \tcontrastive loss: 1.839917\n",
      "Epoch: 772 \tTrain Loss: 3.467691 \tmse loss: 1.600449 \tmse2 loss: 1.170367 \tcontrastive loss: 0.696875\n",
      "Epoch: 773 \tTrain Loss: 3.696275 \tmse loss: 1.572874 \tmse2 loss: 1.147688 \tcontrastive loss: 0.975713\n",
      "Epoch: 774 \tTrain Loss: 3.914614 \tmse loss: 1.583778 \tmse2 loss: 1.219350 \tcontrastive loss: 1.111487\n",
      "Epoch: 775 \tTrain Loss: 2.886553 \tmse loss: 1.526458 \tmse2 loss: 1.052654 \tcontrastive loss: 0.307440\n",
      "Saving..\n",
      "Epoch: 776 \tTrain Loss: 3.360818 \tmse loss: 1.522910 \tmse2 loss: 1.170670 \tcontrastive loss: 0.667238\n",
      "Epoch: 777 \tTrain Loss: 2.950792 \tmse loss: 1.530064 \tmse2 loss: 1.148599 \tcontrastive loss: 0.272129\n",
      "Epoch: 778 \tTrain Loss: 3.017528 \tmse loss: 1.510804 \tmse2 loss: 1.095071 \tcontrastive loss: 0.411654\n",
      "Epoch: 779 \tTrain Loss: 2.995849 \tmse loss: 1.539291 \tmse2 loss: 1.201280 \tcontrastive loss: 0.255278\n",
      "Epoch: 780 \tTrain Loss: 3.743924 \tmse loss: 1.545137 \tmse2 loss: 1.183187 \tcontrastive loss: 1.015600\n",
      "Epoch: 781 \tTrain Loss: 4.083388 \tmse loss: 1.605696 \tmse2 loss: 1.187180 \tcontrastive loss: 1.290512\n",
      "Epoch: 782 \tTrain Loss: 3.551322 \tmse loss: 1.577807 \tmse2 loss: 1.134788 \tcontrastive loss: 0.838726\n",
      "Epoch: 783 \tTrain Loss: 3.604248 \tmse loss: 1.565356 \tmse2 loss: 1.222638 \tcontrastive loss: 0.816254\n",
      "Epoch: 784 \tTrain Loss: 4.566250 \tmse loss: 1.589680 \tmse2 loss: 1.187045 \tcontrastive loss: 1.789525\n",
      "Epoch: 785 \tTrain Loss: 4.873824 \tmse loss: 1.597877 \tmse2 loss: 1.308625 \tcontrastive loss: 1.967322\n",
      "Epoch: 786 \tTrain Loss: 3.953402 \tmse loss: 1.591409 \tmse2 loss: 1.161214 \tcontrastive loss: 1.200779\n",
      "Epoch: 787 \tTrain Loss: 4.052654 \tmse loss: 1.596319 \tmse2 loss: 1.133497 \tcontrastive loss: 1.322839\n",
      "Epoch: 788 \tTrain Loss: 3.988966 \tmse loss: 1.530671 \tmse2 loss: 1.099057 \tcontrastive loss: 1.359238\n",
      "Epoch: 789 \tTrain Loss: 4.083407 \tmse loss: 1.543231 \tmse2 loss: 1.176735 \tcontrastive loss: 1.363441\n",
      "Epoch: 790 \tTrain Loss: 3.311797 \tmse loss: 1.561607 \tmse2 loss: 1.131568 \tcontrastive loss: 0.618621\n",
      "Epoch: 791 \tTrain Loss: 3.979379 \tmse loss: 1.575551 \tmse2 loss: 1.166805 \tcontrastive loss: 1.237022\n",
      "Epoch: 792 \tTrain Loss: 4.717154 \tmse loss: 1.604036 \tmse2 loss: 1.154683 \tcontrastive loss: 1.958435\n",
      "Epoch: 793 \tTrain Loss: 3.046692 \tmse loss: 1.546273 \tmse2 loss: 1.182564 \tcontrastive loss: 0.317855\n",
      "Epoch: 794 \tTrain Loss: 3.782504 \tmse loss: 1.557183 \tmse2 loss: 1.067755 \tcontrastive loss: 1.157566\n",
      "Epoch: 795 \tTrain Loss: 3.683646 \tmse loss: 1.571896 \tmse2 loss: 1.097936 \tcontrastive loss: 1.013814\n",
      "Epoch: 796 \tTrain Loss: 3.137474 \tmse loss: 1.506079 \tmse2 loss: 1.137539 \tcontrastive loss: 0.493856\n",
      "Epoch: 797 \tTrain Loss: 2.997803 \tmse loss: 1.476994 \tmse2 loss: 1.102051 \tcontrastive loss: 0.418757\n",
      "Epoch: 798 \tTrain Loss: 2.907850 \tmse loss: 1.481134 \tmse2 loss: 1.073446 \tcontrastive loss: 0.353270\n",
      "Epoch: 799 \tTrain Loss: 3.589596 \tmse loss: 1.525004 \tmse2 loss: 1.125722 \tcontrastive loss: 0.938869\n",
      "Epoch: 800 \tTrain Loss: 3.581096 \tmse loss: 1.527438 \tmse2 loss: 1.038751 \tcontrastive loss: 1.014907\n",
      "Epoch: 801 \tTrain Loss: 4.395311 \tmse loss: 1.543903 \tmse2 loss: 1.187322 \tcontrastive loss: 1.664086\n",
      "Epoch: 802 \tTrain Loss: 3.984347 \tmse loss: 1.566479 \tmse2 loss: 1.237568 \tcontrastive loss: 1.180301\n",
      "Epoch: 803 \tTrain Loss: 3.926058 \tmse loss: 1.548511 \tmse2 loss: 1.222898 \tcontrastive loss: 1.154648\n",
      "Epoch: 804 \tTrain Loss: 3.636903 \tmse loss: 1.512859 \tmse2 loss: 1.226344 \tcontrastive loss: 0.897699\n",
      "Epoch: 805 \tTrain Loss: 3.018158 \tmse loss: 1.459807 \tmse2 loss: 1.172800 \tcontrastive loss: 0.385550\n",
      "Epoch: 806 \tTrain Loss: 3.226642 \tmse loss: 1.483242 \tmse2 loss: 1.155345 \tcontrastive loss: 0.588056\n",
      "Epoch: 807 \tTrain Loss: 3.768168 \tmse loss: 1.508805 \tmse2 loss: 1.062028 \tcontrastive loss: 1.197335\n",
      "Epoch: 808 \tTrain Loss: 3.906777 \tmse loss: 1.507095 \tmse2 loss: 1.213863 \tcontrastive loss: 1.185819\n",
      "Epoch: 809 \tTrain Loss: 3.458468 \tmse loss: 1.529772 \tmse2 loss: 1.051327 \tcontrastive loss: 0.877370\n",
      "Epoch: 810 \tTrain Loss: 3.397163 \tmse loss: 1.534077 \tmse2 loss: 1.130321 \tcontrastive loss: 0.732766\n",
      "Epoch: 811 \tTrain Loss: 3.656626 \tmse loss: 1.537246 \tmse2 loss: 1.110338 \tcontrastive loss: 1.009043\n",
      "Epoch: 812 \tTrain Loss: 2.887518 \tmse loss: 1.493655 \tmse2 loss: 1.071870 \tcontrastive loss: 0.321992\n",
      "Epoch: 813 \tTrain Loss: 2.840125 \tmse loss: 1.483119 \tmse2 loss: 1.104328 \tcontrastive loss: 0.252678\n",
      "Saving..\n",
      "Epoch: 814 \tTrain Loss: 3.076161 \tmse loss: 1.447950 \tmse2 loss: 1.030299 \tcontrastive loss: 0.597912\n",
      "Epoch: 815 \tTrain Loss: 3.234150 \tmse loss: 1.516566 \tmse2 loss: 1.138781 \tcontrastive loss: 0.578803\n",
      "Epoch: 816 \tTrain Loss: 4.644100 \tmse loss: 1.509733 \tmse2 loss: 1.104560 \tcontrastive loss: 2.029806\n",
      "Epoch: 817 \tTrain Loss: 4.909882 \tmse loss: 1.561335 \tmse2 loss: 1.169572 \tcontrastive loss: 2.178975\n",
      "Epoch: 818 \tTrain Loss: 3.908963 \tmse loss: 1.565618 \tmse2 loss: 1.181848 \tcontrastive loss: 1.161496\n",
      "Epoch: 819 \tTrain Loss: 4.008161 \tmse loss: 1.532512 \tmse2 loss: 1.118883 \tcontrastive loss: 1.356766\n",
      "Epoch: 820 \tTrain Loss: 3.327608 \tmse loss: 1.504312 \tmse2 loss: 1.037223 \tcontrastive loss: 0.786073\n",
      "Epoch: 821 \tTrain Loss: 3.537339 \tmse loss: 1.506513 \tmse2 loss: 1.114894 \tcontrastive loss: 0.915932\n",
      "Epoch: 822 \tTrain Loss: 3.414962 \tmse loss: 1.480427 \tmse2 loss: 1.110753 \tcontrastive loss: 0.823782\n",
      "Epoch: 823 \tTrain Loss: 3.846850 \tmse loss: 1.544268 \tmse2 loss: 1.094253 \tcontrastive loss: 1.208329\n",
      "Epoch: 824 \tTrain Loss: 3.675463 \tmse loss: 1.525072 \tmse2 loss: 1.190973 \tcontrastive loss: 0.959418\n",
      "Epoch: 825 \tTrain Loss: 3.569351 \tmse loss: 1.493325 \tmse2 loss: 1.086226 \tcontrastive loss: 0.989800\n",
      "Epoch: 826 \tTrain Loss: 3.416262 \tmse loss: 1.522641 \tmse2 loss: 1.172880 \tcontrastive loss: 0.720740\n",
      "Epoch: 827 \tTrain Loss: 3.355683 \tmse loss: 1.491090 \tmse2 loss: 1.099526 \tcontrastive loss: 0.765068\n",
      "Epoch: 828 \tTrain Loss: 3.168937 \tmse loss: 1.486792 \tmse2 loss: 1.115573 \tcontrastive loss: 0.566572\n",
      "Epoch: 829 \tTrain Loss: 3.533755 \tmse loss: 1.464074 \tmse2 loss: 1.031653 \tcontrastive loss: 1.038028\n",
      "Epoch: 830 \tTrain Loss: 3.423127 \tmse loss: 1.457692 \tmse2 loss: 1.038038 \tcontrastive loss: 0.927398\n",
      "Epoch: 831 \tTrain Loss: 2.803719 \tmse loss: 1.476712 \tmse2 loss: 1.102557 \tcontrastive loss: 0.224450\n",
      "Saving..\n",
      "Epoch: 832 \tTrain Loss: 2.832607 \tmse loss: 1.446211 \tmse2 loss: 1.126201 \tcontrastive loss: 0.260196\n",
      "Epoch: 833 \tTrain Loss: 3.624587 \tmse loss: 1.426830 \tmse2 loss: 1.085591 \tcontrastive loss: 1.112166\n",
      "Epoch: 834 \tTrain Loss: 3.161334 \tmse loss: 1.440477 \tmse2 loss: 1.092312 \tcontrastive loss: 0.628546\n",
      "Epoch: 835 \tTrain Loss: 3.876710 \tmse loss: 1.492387 \tmse2 loss: 1.116661 \tcontrastive loss: 1.267662\n",
      "Epoch: 836 \tTrain Loss: 4.024308 \tmse loss: 1.473280 \tmse2 loss: 1.054764 \tcontrastive loss: 1.496264\n",
      "Epoch: 837 \tTrain Loss: 4.178122 \tmse loss: 1.466692 \tmse2 loss: 1.060558 \tcontrastive loss: 1.650872\n",
      "Epoch: 838 \tTrain Loss: 3.485530 \tmse loss: 1.491059 \tmse2 loss: 1.168143 \tcontrastive loss: 0.826328\n",
      "Epoch: 839 \tTrain Loss: 3.748142 \tmse loss: 1.502519 \tmse2 loss: 1.073018 \tcontrastive loss: 1.172606\n",
      "Epoch: 840 \tTrain Loss: 3.813616 \tmse loss: 1.481796 \tmse2 loss: 1.087353 \tcontrastive loss: 1.244467\n",
      "Epoch: 841 \tTrain Loss: 3.592649 \tmse loss: 1.510493 \tmse2 loss: 1.040337 \tcontrastive loss: 1.041818\n",
      "Epoch: 842 \tTrain Loss: 4.101307 \tmse loss: 1.526496 \tmse2 loss: 1.078953 \tcontrastive loss: 1.495858\n",
      "Epoch: 843 \tTrain Loss: 3.615346 \tmse loss: 1.493973 \tmse2 loss: 1.130395 \tcontrastive loss: 0.990978\n",
      "Epoch: 844 \tTrain Loss: 3.544558 \tmse loss: 1.491128 \tmse2 loss: 1.008653 \tcontrastive loss: 1.044777\n",
      "Epoch: 845 \tTrain Loss: 3.501162 \tmse loss: 1.510655 \tmse2 loss: 1.202689 \tcontrastive loss: 0.787818\n",
      "Epoch: 846 \tTrain Loss: 3.009125 \tmse loss: 1.479996 \tmse2 loss: 1.151819 \tcontrastive loss: 0.377310\n",
      "Epoch: 847 \tTrain Loss: 2.814811 \tmse loss: 1.462972 \tmse2 loss: 1.024226 \tcontrastive loss: 0.327612\n",
      "Epoch: 848 \tTrain Loss: 2.756268 \tmse loss: 1.468154 \tmse2 loss: 1.064708 \tcontrastive loss: 0.223407\n",
      "Saving..\n",
      "Epoch: 849 \tTrain Loss: 2.713619 \tmse loss: 1.447452 \tmse2 loss: 1.032755 \tcontrastive loss: 0.233412\n",
      "Saving..\n",
      "Epoch: 850 \tTrain Loss: 3.294434 \tmse loss: 1.417354 \tmse2 loss: 1.078592 \tcontrastive loss: 0.798487\n",
      "Epoch: 851 \tTrain Loss: 3.477962 \tmse loss: 1.399245 \tmse2 loss: 0.984147 \tcontrastive loss: 1.094570\n",
      "Epoch: 852 \tTrain Loss: 2.936695 \tmse loss: 1.414131 \tmse2 loss: 1.064930 \tcontrastive loss: 0.457633\n",
      "Epoch: 853 \tTrain Loss: 3.016226 \tmse loss: 1.427819 \tmse2 loss: 1.037899 \tcontrastive loss: 0.550509\n",
      "Epoch: 854 \tTrain Loss: 3.649127 \tmse loss: 1.420549 \tmse2 loss: 1.058068 \tcontrastive loss: 1.170511\n",
      "Epoch: 855 \tTrain Loss: 3.686726 \tmse loss: 1.436128 \tmse2 loss: 1.138729 \tcontrastive loss: 1.111870\n",
      "Epoch: 856 \tTrain Loss: 3.270793 \tmse loss: 1.477336 \tmse2 loss: 1.133059 \tcontrastive loss: 0.660399\n",
      "Epoch: 857 \tTrain Loss: 4.047104 \tmse loss: 1.461797 \tmse2 loss: 1.133028 \tcontrastive loss: 1.452279\n",
      "Epoch: 858 \tTrain Loss: 4.574805 \tmse loss: 1.521545 \tmse2 loss: 1.128288 \tcontrastive loss: 1.924972\n",
      "Epoch: 859 \tTrain Loss: 4.013943 \tmse loss: 1.557416 \tmse2 loss: 1.159661 \tcontrastive loss: 1.296866\n",
      "Epoch: 860 \tTrain Loss: 3.161867 \tmse loss: 1.497909 \tmse2 loss: 1.133257 \tcontrastive loss: 0.530701\n",
      "Epoch: 861 \tTrain Loss: 3.117307 \tmse loss: 1.475631 \tmse2 loss: 1.081530 \tcontrastive loss: 0.560147\n",
      "Epoch: 862 \tTrain Loss: 3.644399 \tmse loss: 1.444670 \tmse2 loss: 1.143311 \tcontrastive loss: 1.056417\n",
      "Epoch: 863 \tTrain Loss: 3.945125 \tmse loss: 1.500695 \tmse2 loss: 1.120117 \tcontrastive loss: 1.324313\n",
      "Epoch: 864 \tTrain Loss: 3.914232 \tmse loss: 1.520569 \tmse2 loss: 1.073057 \tcontrastive loss: 1.320607\n",
      "Epoch: 865 \tTrain Loss: 3.494872 \tmse loss: 1.495022 \tmse2 loss: 1.101442 \tcontrastive loss: 0.898408\n",
      "Epoch: 866 \tTrain Loss: 3.448794 \tmse loss: 1.534515 \tmse2 loss: 1.101621 \tcontrastive loss: 0.812658\n",
      "Epoch: 867 \tTrain Loss: 3.584861 \tmse loss: 1.505223 \tmse2 loss: 1.018055 \tcontrastive loss: 1.061584\n",
      "Epoch: 868 \tTrain Loss: 3.444768 \tmse loss: 1.475445 \tmse2 loss: 1.090998 \tcontrastive loss: 0.878325\n",
      "Epoch: 869 \tTrain Loss: 3.041550 \tmse loss: 1.431202 \tmse2 loss: 1.052003 \tcontrastive loss: 0.558345\n",
      "Epoch: 870 \tTrain Loss: 2.731460 \tmse loss: 1.387498 \tmse2 loss: 1.104388 \tcontrastive loss: 0.239574\n",
      "Epoch: 871 \tTrain Loss: 3.319160 \tmse loss: 1.450886 \tmse2 loss: 1.062223 \tcontrastive loss: 0.806051\n",
      "Epoch: 872 \tTrain Loss: 3.274214 \tmse loss: 1.476277 \tmse2 loss: 1.071309 \tcontrastive loss: 0.726628\n",
      "Epoch: 873 \tTrain Loss: 3.130316 \tmse loss: 1.426480 \tmse2 loss: 1.081591 \tcontrastive loss: 0.622245\n",
      "Epoch: 874 \tTrain Loss: 3.916777 \tmse loss: 1.427182 \tmse2 loss: 0.970351 \tcontrastive loss: 1.519243\n",
      "Epoch: 875 \tTrain Loss: 3.102602 \tmse loss: 1.468232 \tmse2 loss: 1.038594 \tcontrastive loss: 0.595776\n",
      "Epoch: 876 \tTrain Loss: 4.246382 \tmse loss: 1.486266 \tmse2 loss: 1.126520 \tcontrastive loss: 1.633596\n",
      "Epoch: 877 \tTrain Loss: 3.563377 \tmse loss: 1.479270 \tmse2 loss: 1.072199 \tcontrastive loss: 1.011908\n",
      "Epoch: 878 \tTrain Loss: 3.536190 \tmse loss: 1.462595 \tmse2 loss: 1.089550 \tcontrastive loss: 0.984045\n",
      "Epoch: 879 \tTrain Loss: 3.562829 \tmse loss: 1.464676 \tmse2 loss: 1.038737 \tcontrastive loss: 1.059416\n",
      "Epoch: 880 \tTrain Loss: 2.977094 \tmse loss: 1.451072 \tmse2 loss: 1.030059 \tcontrastive loss: 0.495962\n",
      "Epoch: 881 \tTrain Loss: 2.935396 \tmse loss: 1.441943 \tmse2 loss: 1.078319 \tcontrastive loss: 0.415135\n",
      "Epoch: 882 \tTrain Loss: 2.860485 \tmse loss: 1.428998 \tmse2 loss: 1.009326 \tcontrastive loss: 0.422160\n",
      "Epoch: 883 \tTrain Loss: 3.027824 \tmse loss: 1.436077 \tmse2 loss: 1.066342 \tcontrastive loss: 0.525405\n",
      "Epoch: 884 \tTrain Loss: 3.357356 \tmse loss: 1.432616 \tmse2 loss: 1.093355 \tcontrastive loss: 0.831386\n",
      "Epoch: 885 \tTrain Loss: 3.374316 \tmse loss: 1.429612 \tmse2 loss: 1.133527 \tcontrastive loss: 0.811177\n",
      "Epoch: 886 \tTrain Loss: 3.308192 \tmse loss: 1.462491 \tmse2 loss: 1.075053 \tcontrastive loss: 0.770648\n",
      "Epoch: 887 \tTrain Loss: 2.866655 \tmse loss: 1.407064 \tmse2 loss: 1.081410 \tcontrastive loss: 0.378181\n",
      "Epoch: 888 \tTrain Loss: 2.743518 \tmse loss: 1.387551 \tmse2 loss: 1.019866 \tcontrastive loss: 0.336102\n",
      "Epoch: 889 \tTrain Loss: 3.168364 \tmse loss: 1.416047 \tmse2 loss: 1.000815 \tcontrastive loss: 0.751502\n",
      "Epoch: 890 \tTrain Loss: 3.065991 \tmse loss: 1.440485 \tmse2 loss: 1.057252 \tcontrastive loss: 0.568254\n",
      "Epoch: 891 \tTrain Loss: 3.246224 \tmse loss: 1.431283 \tmse2 loss: 0.982528 \tcontrastive loss: 0.832412\n",
      "Epoch: 892 \tTrain Loss: 3.371465 \tmse loss: 1.406651 \tmse2 loss: 1.044288 \tcontrastive loss: 0.920526\n",
      "Epoch: 893 \tTrain Loss: 4.055929 \tmse loss: 1.487832 \tmse2 loss: 1.159184 \tcontrastive loss: 1.408913\n",
      "Epoch: 894 \tTrain Loss: 3.581985 \tmse loss: 1.481703 \tmse2 loss: 1.079362 \tcontrastive loss: 1.020921\n",
      "Epoch: 895 \tTrain Loss: 2.982000 \tmse loss: 1.437502 \tmse2 loss: 1.022851 \tcontrastive loss: 0.521647\n",
      "Epoch: 896 \tTrain Loss: 3.151410 \tmse loss: 1.422474 \tmse2 loss: 1.035073 \tcontrastive loss: 0.693864\n",
      "Epoch: 897 \tTrain Loss: 3.475345 \tmse loss: 1.433888 \tmse2 loss: 1.072030 \tcontrastive loss: 0.969427\n",
      "Epoch: 898 \tTrain Loss: 2.738780 \tmse loss: 1.440005 \tmse2 loss: 1.019067 \tcontrastive loss: 0.279708\n",
      "Epoch: 899 \tTrain Loss: 3.657311 \tmse loss: 1.490408 \tmse2 loss: 1.130086 \tcontrastive loss: 1.036817\n",
      "Epoch: 900 \tTrain Loss: 3.082870 \tmse loss: 1.440047 \tmse2 loss: 1.021494 \tcontrastive loss: 0.621328\n",
      "Epoch: 901 \tTrain Loss: 3.125913 \tmse loss: 1.414523 \tmse2 loss: 1.101539 \tcontrastive loss: 0.609850\n",
      "Epoch: 902 \tTrain Loss: 3.528166 \tmse loss: 1.421334 \tmse2 loss: 1.083223 \tcontrastive loss: 1.023609\n",
      "Epoch: 903 \tTrain Loss: 2.991985 \tmse loss: 1.403452 \tmse2 loss: 1.132236 \tcontrastive loss: 0.456297\n",
      "Epoch: 904 \tTrain Loss: 3.180171 \tmse loss: 1.420848 \tmse2 loss: 1.062052 \tcontrastive loss: 0.697271\n",
      "Epoch: 905 \tTrain Loss: 3.014489 \tmse loss: 1.421501 \tmse2 loss: 1.018236 \tcontrastive loss: 0.574752\n",
      "Epoch: 906 \tTrain Loss: 3.276301 \tmse loss: 1.436841 \tmse2 loss: 1.104732 \tcontrastive loss: 0.734729\n",
      "Epoch: 907 \tTrain Loss: 3.287581 \tmse loss: 1.413100 \tmse2 loss: 1.065714 \tcontrastive loss: 0.808767\n",
      "Epoch: 908 \tTrain Loss: 4.006514 \tmse loss: 1.418090 \tmse2 loss: 1.068095 \tcontrastive loss: 1.520329\n",
      "Epoch: 909 \tTrain Loss: 3.602155 \tmse loss: 1.412159 \tmse2 loss: 0.996845 \tcontrastive loss: 1.193151\n",
      "Epoch: 910 \tTrain Loss: 2.965813 \tmse loss: 1.417386 \tmse2 loss: 0.978786 \tcontrastive loss: 0.569641\n",
      "Epoch: 911 \tTrain Loss: 4.157536 \tmse loss: 1.412248 \tmse2 loss: 0.995474 \tcontrastive loss: 1.749815\n",
      "Epoch: 912 \tTrain Loss: 2.998465 \tmse loss: 1.419521 \tmse2 loss: 0.964119 \tcontrastive loss: 0.614824\n",
      "Epoch: 913 \tTrain Loss: 3.278551 \tmse loss: 1.392064 \tmse2 loss: 1.085154 \tcontrastive loss: 0.801334\n",
      "Epoch: 914 \tTrain Loss: 3.255205 \tmse loss: 1.453284 \tmse2 loss: 1.044308 \tcontrastive loss: 0.757613\n",
      "Epoch: 915 \tTrain Loss: 2.921795 \tmse loss: 1.388825 \tmse2 loss: 0.945263 \tcontrastive loss: 0.587707\n",
      "Epoch: 916 \tTrain Loss: 3.397163 \tmse loss: 1.444931 \tmse2 loss: 1.123331 \tcontrastive loss: 0.828901\n",
      "Epoch: 917 \tTrain Loss: 3.390478 \tmse loss: 1.404605 \tmse2 loss: 1.055496 \tcontrastive loss: 0.930377\n",
      "Epoch: 918 \tTrain Loss: 3.548991 \tmse loss: 1.394415 \tmse2 loss: 1.084365 \tcontrastive loss: 1.070211\n",
      "Epoch: 919 \tTrain Loss: 2.898082 \tmse loss: 1.374556 \tmse2 loss: 1.069885 \tcontrastive loss: 0.453640\n",
      "Epoch: 920 \tTrain Loss: 3.008904 \tmse loss: 1.351201 \tmse2 loss: 1.026075 \tcontrastive loss: 0.631628\n",
      "Epoch: 921 \tTrain Loss: 2.798111 \tmse loss: 1.385387 \tmse2 loss: 1.033676 \tcontrastive loss: 0.379048\n",
      "Epoch: 922 \tTrain Loss: 3.347114 \tmse loss: 1.405358 \tmse2 loss: 1.015840 \tcontrastive loss: 0.925917\n",
      "Epoch: 923 \tTrain Loss: 2.751263 \tmse loss: 1.337354 \tmse2 loss: 1.012206 \tcontrastive loss: 0.401703\n",
      "Epoch: 924 \tTrain Loss: 3.191015 \tmse loss: 1.373125 \tmse2 loss: 1.059044 \tcontrastive loss: 0.758846\n",
      "Epoch: 925 \tTrain Loss: 3.706316 \tmse loss: 1.398736 \tmse2 loss: 1.004394 \tcontrastive loss: 1.303186\n",
      "Epoch: 926 \tTrain Loss: 3.095846 \tmse loss: 1.414649 \tmse2 loss: 1.038598 \tcontrastive loss: 0.642600\n",
      "Epoch: 927 \tTrain Loss: 3.020433 \tmse loss: 1.367775 \tmse2 loss: 0.994356 \tcontrastive loss: 0.658301\n",
      "Epoch: 928 \tTrain Loss: 3.137838 \tmse loss: 1.387671 \tmse2 loss: 1.000978 \tcontrastive loss: 0.749189\n",
      "Epoch: 929 \tTrain Loss: 3.204786 \tmse loss: 1.398890 \tmse2 loss: 1.072579 \tcontrastive loss: 0.733317\n",
      "Epoch: 930 \tTrain Loss: 3.893519 \tmse loss: 1.388674 \tmse2 loss: 1.061535 \tcontrastive loss: 1.443309\n",
      "Epoch: 931 \tTrain Loss: 4.405620 \tmse loss: 1.417931 \tmse2 loss: 1.066333 \tcontrastive loss: 1.921356\n",
      "Epoch: 932 \tTrain Loss: 3.390601 \tmse loss: 1.416857 \tmse2 loss: 1.052879 \tcontrastive loss: 0.920865\n",
      "Epoch: 933 \tTrain Loss: 2.860647 \tmse loss: 1.411934 \tmse2 loss: 1.050659 \tcontrastive loss: 0.398053\n",
      "Epoch: 934 \tTrain Loss: 2.353008 \tmse loss: 1.347596 \tmse2 loss: 0.909178 \tcontrastive loss: 0.096234\n",
      "Saving..\n",
      "Epoch: 935 \tTrain Loss: 2.943416 \tmse loss: 1.354460 \tmse2 loss: 0.938395 \tcontrastive loss: 0.650562\n",
      "Epoch: 936 \tTrain Loss: 2.947763 \tmse loss: 1.352938 \tmse2 loss: 1.032003 \tcontrastive loss: 0.562822\n",
      "Epoch: 937 \tTrain Loss: 2.680034 \tmse loss: 1.360293 \tmse2 loss: 0.982769 \tcontrastive loss: 0.336973\n",
      "Epoch: 938 \tTrain Loss: 3.041271 \tmse loss: 1.374323 \tmse2 loss: 1.039593 \tcontrastive loss: 0.627354\n",
      "Epoch: 939 \tTrain Loss: 4.241050 \tmse loss: 1.400642 \tmse2 loss: 1.098689 \tcontrastive loss: 1.741718\n",
      "Epoch: 940 \tTrain Loss: 3.608135 \tmse loss: 1.391803 \tmse2 loss: 1.094177 \tcontrastive loss: 1.122155\n",
      "Epoch: 941 \tTrain Loss: 3.401898 \tmse loss: 1.408986 \tmse2 loss: 1.042922 \tcontrastive loss: 0.949990\n",
      "Epoch: 942 \tTrain Loss: 3.277634 \tmse loss: 1.376329 \tmse2 loss: 0.995968 \tcontrastive loss: 0.905338\n",
      "Epoch: 943 \tTrain Loss: 2.822287 \tmse loss: 1.393055 \tmse2 loss: 0.977436 \tcontrastive loss: 0.451797\n",
      "Epoch: 944 \tTrain Loss: 3.439296 \tmse loss: 1.403005 \tmse2 loss: 1.084941 \tcontrastive loss: 0.951350\n",
      "Epoch: 945 \tTrain Loss: 3.022452 \tmse loss: 1.384478 \tmse2 loss: 0.977342 \tcontrastive loss: 0.660631\n",
      "Epoch: 946 \tTrain Loss: 3.229111 \tmse loss: 1.360852 \tmse2 loss: 0.993221 \tcontrastive loss: 0.875038\n",
      "Epoch: 947 \tTrain Loss: 3.140221 \tmse loss: 1.363658 \tmse2 loss: 1.001300 \tcontrastive loss: 0.775263\n",
      "Epoch: 948 \tTrain Loss: 3.083501 \tmse loss: 1.382134 \tmse2 loss: 1.057605 \tcontrastive loss: 0.643762\n",
      "Epoch: 949 \tTrain Loss: 2.837674 \tmse loss: 1.382592 \tmse2 loss: 0.944964 \tcontrastive loss: 0.510118\n",
      "Epoch: 950 \tTrain Loss: 3.355080 \tmse loss: 1.410433 \tmse2 loss: 1.077802 \tcontrastive loss: 0.866845\n",
      "Epoch: 951 \tTrain Loss: 2.928412 \tmse loss: 1.365244 \tmse2 loss: 0.936283 \tcontrastive loss: 0.626885\n",
      "Epoch: 952 \tTrain Loss: 2.700254 \tmse loss: 1.351398 \tmse2 loss: 0.959630 \tcontrastive loss: 0.389226\n",
      "Epoch: 953 \tTrain Loss: 3.024735 \tmse loss: 1.349284 \tmse2 loss: 0.950148 \tcontrastive loss: 0.725304\n",
      "Epoch: 954 \tTrain Loss: 2.763046 \tmse loss: 1.343903 \tmse2 loss: 1.005022 \tcontrastive loss: 0.414121\n",
      "Epoch: 955 \tTrain Loss: 3.211073 \tmse loss: 1.358598 \tmse2 loss: 0.989346 \tcontrastive loss: 0.863129\n",
      "Epoch: 956 \tTrain Loss: 2.883485 \tmse loss: 1.354959 \tmse2 loss: 0.996794 \tcontrastive loss: 0.531732\n",
      "Epoch: 957 \tTrain Loss: 3.350580 \tmse loss: 1.395401 \tmse2 loss: 1.004608 \tcontrastive loss: 0.950571\n",
      "Epoch: 958 \tTrain Loss: 2.970235 \tmse loss: 1.370754 \tmse2 loss: 0.997105 \tcontrastive loss: 0.602376\n",
      "Epoch: 959 \tTrain Loss: 3.099960 \tmse loss: 1.370893 \tmse2 loss: 1.023012 \tcontrastive loss: 0.706055\n",
      "Epoch: 960 \tTrain Loss: 3.406660 \tmse loss: 1.407487 \tmse2 loss: 1.076080 \tcontrastive loss: 0.923093\n",
      "Epoch: 961 \tTrain Loss: 3.178817 \tmse loss: 1.407725 \tmse2 loss: 0.999884 \tcontrastive loss: 0.771208\n",
      "Epoch: 962 \tTrain Loss: 3.251082 \tmse loss: 1.401228 \tmse2 loss: 1.030951 \tcontrastive loss: 0.818903\n",
      "Epoch: 963 \tTrain Loss: 3.129984 \tmse loss: 1.405139 \tmse2 loss: 1.054533 \tcontrastive loss: 0.670313\n",
      "Epoch: 964 \tTrain Loss: 3.185596 \tmse loss: 1.358503 \tmse2 loss: 0.982923 \tcontrastive loss: 0.844171\n",
      "Epoch: 965 \tTrain Loss: 3.189685 \tmse loss: 1.398206 \tmse2 loss: 1.061528 \tcontrastive loss: 0.729951\n",
      "Epoch: 966 \tTrain Loss: 3.861633 \tmse loss: 1.423182 \tmse2 loss: 1.027488 \tcontrastive loss: 1.410962\n",
      "Epoch: 967 \tTrain Loss: 3.396703 \tmse loss: 1.399276 \tmse2 loss: 1.055777 \tcontrastive loss: 0.941651\n",
      "Epoch: 968 \tTrain Loss: 3.932645 \tmse loss: 1.395234 \tmse2 loss: 1.051583 \tcontrastive loss: 1.485827\n",
      "Epoch: 969 \tTrain Loss: 2.907950 \tmse loss: 1.410392 \tmse2 loss: 1.083480 \tcontrastive loss: 0.414077\n",
      "Epoch: 970 \tTrain Loss: 3.381955 \tmse loss: 1.377134 \tmse2 loss: 1.012159 \tcontrastive loss: 0.992662\n",
      "Epoch: 971 \tTrain Loss: 3.490201 \tmse loss: 1.336393 \tmse2 loss: 1.096445 \tcontrastive loss: 1.057363\n",
      "Epoch: 972 \tTrain Loss: 3.191774 \tmse loss: 1.368149 \tmse2 loss: 1.045576 \tcontrastive loss: 0.778048\n",
      "Epoch: 973 \tTrain Loss: 2.832317 \tmse loss: 1.361076 \tmse2 loss: 1.017531 \tcontrastive loss: 0.453710\n",
      "Epoch: 974 \tTrain Loss: 2.919134 \tmse loss: 1.321145 \tmse2 loss: 0.952772 \tcontrastive loss: 0.645217\n",
      "Epoch: 975 \tTrain Loss: 2.806493 \tmse loss: 1.343839 \tmse2 loss: 0.964653 \tcontrastive loss: 0.498001\n",
      "Epoch: 976 \tTrain Loss: 2.909679 \tmse loss: 1.306649 \tmse2 loss: 0.997496 \tcontrastive loss: 0.605534\n",
      "Epoch: 977 \tTrain Loss: 3.032312 \tmse loss: 1.321997 \tmse2 loss: 1.004666 \tcontrastive loss: 0.705650\n",
      "Epoch: 978 \tTrain Loss: 2.663136 \tmse loss: 1.319336 \tmse2 loss: 0.993540 \tcontrastive loss: 0.350261\n",
      "Epoch: 979 \tTrain Loss: 2.928478 \tmse loss: 1.309753 \tmse2 loss: 0.983861 \tcontrastive loss: 0.634863\n",
      "Epoch: 980 \tTrain Loss: 3.015896 \tmse loss: 1.347714 \tmse2 loss: 1.039050 \tcontrastive loss: 0.629133\n",
      "Epoch: 981 \tTrain Loss: 2.652716 \tmse loss: 1.286029 \tmse2 loss: 0.996574 \tcontrastive loss: 0.370114\n",
      "Epoch: 982 \tTrain Loss: 3.093262 \tmse loss: 1.284053 \tmse2 loss: 0.977761 \tcontrastive loss: 0.831448\n",
      "Epoch: 983 \tTrain Loss: 3.504022 \tmse loss: 1.296487 \tmse2 loss: 0.988927 \tcontrastive loss: 1.218608\n",
      "Epoch: 984 \tTrain Loss: 3.276960 \tmse loss: 1.387165 \tmse2 loss: 1.088541 \tcontrastive loss: 0.801254\n",
      "Epoch: 985 \tTrain Loss: 3.211542 \tmse loss: 1.361565 \tmse2 loss: 1.023423 \tcontrastive loss: 0.826554\n",
      "Epoch: 986 \tTrain Loss: 2.637240 \tmse loss: 1.322638 \tmse2 loss: 0.928075 \tcontrastive loss: 0.386527\n",
      "Epoch: 987 \tTrain Loss: 3.009253 \tmse loss: 1.343440 \tmse2 loss: 0.936055 \tcontrastive loss: 0.729759\n",
      "Epoch: 988 \tTrain Loss: 2.863201 \tmse loss: 1.345353 \tmse2 loss: 1.035168 \tcontrastive loss: 0.482681\n",
      "Epoch: 989 \tTrain Loss: 2.441031 \tmse loss: 1.306915 \tmse2 loss: 0.931777 \tcontrastive loss: 0.202338\n",
      "Epoch: 990 \tTrain Loss: 3.022590 \tmse loss: 1.298272 \tmse2 loss: 0.926076 \tcontrastive loss: 0.798241\n",
      "Epoch: 991 \tTrain Loss: 2.543978 \tmse loss: 1.295246 \tmse2 loss: 0.887754 \tcontrastive loss: 0.360978\n",
      "Epoch: 992 \tTrain Loss: 3.452540 \tmse loss: 1.327283 \tmse2 loss: 1.011468 \tcontrastive loss: 1.113789\n",
      "Epoch: 993 \tTrain Loss: 3.379431 \tmse loss: 1.349728 \tmse2 loss: 1.095347 \tcontrastive loss: 0.934356\n",
      "Epoch: 994 \tTrain Loss: 3.632188 \tmse loss: 1.379392 \tmse2 loss: 1.069222 \tcontrastive loss: 1.183574\n",
      "Epoch: 995 \tTrain Loss: 2.824652 \tmse loss: 1.405734 \tmse2 loss: 1.020353 \tcontrastive loss: 0.398565\n",
      "Epoch: 996 \tTrain Loss: 3.166996 \tmse loss: 1.343566 \tmse2 loss: 1.000610 \tcontrastive loss: 0.822820\n",
      "Epoch: 997 \tTrain Loss: 2.965360 \tmse loss: 1.333206 \tmse2 loss: 0.988960 \tcontrastive loss: 0.643194\n",
      "Epoch: 998 \tTrain Loss: 3.886889 \tmse loss: 1.382577 \tmse2 loss: 0.941014 \tcontrastive loss: 1.563298\n",
      "Epoch: 999 \tTrain Loss: 3.071972 \tmse loss: 1.370479 \tmse2 loss: 0.950985 \tcontrastive loss: 0.750508\n",
      "./0_final_enc.pth\n",
      "./0_final_dec.pth\n",
      "\n",
      "total time(min): 126.40\n",
      "final time(min): 126.98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t3 = time.time()\n",
    "\n",
    "##############################################################################\n",
    "\"\"\"args for AE\"\"\"\n",
    "\n",
    "args = {}\n",
    "args['dim_h'] = 64         # factor controlling size of hidden layers\n",
    "args['n_channel'] = 3#1    # number of channels in the input data \n",
    "\n",
    "args['n_z'] = 600 #300     # number of dimensions in latent space. \n",
    "\n",
    "args['sigma'] = 1.0        # variance in n_z\n",
    "args['lambda'] = 0.01      # hyper param for weight of discriminator loss\n",
    "args['lr'] = 0.0002        # learning rate for Adam optimizer .000\n",
    "args['epochs'] = 1000       # how many epochs to run for\n",
    "args['batch_size'] = 100   # batch size for SGD\n",
    "args['save'] = True        # save weights at each epoch of training if True\n",
    "args['train'] = True       # train networks if True, else load networks from\n",
    "args['triplet_margin'] = 0.4 # default 0.2, use 0.4 to try\n",
    "\n",
    "args['dataset'] = 'cifar10'#'mnist'  #'fmnist' # specify which dataset to use\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "## create encoder model and decoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters, work excellent with image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            #nn.ReLU(True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            #nn.ReLU(True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            #nn.ReLU(True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            \n",
    "            #nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            \n",
    "            \n",
    "            #3d and 32 by 32\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 1, 0, bias=False),\n",
    "            \n",
    "            nn.BatchNorm2d(self.dim_h * 8), # 40 X 8 = 320\n",
    "            #nn.ReLU(True))\n",
    "            nn.LeakyReLU(0.2, inplace=True)) #,\n",
    "            #nn.Conv2d(self.dim_h * 8, 1, 2, 1, 0, bias=False))\n",
    "            #nn.Conv2d(self.dim_h * 8, 1, 4, 1, 0, bias=False))\n",
    "        # final layer is fully connected\n",
    "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('enc')\n",
    "        #print('input ',x.size()) #torch.Size([100, 3,32,32])\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        x = x.squeeze()\n",
    "        #print('aft squeeze ',x.size()) #torch.Size([128, 320])\n",
    "        #aft squeeze  torch.Size([100, 320])\n",
    "        x = self.fc(x)\n",
    "        #print('out ',x.size()) #torch.Size([128, 20])\n",
    "        #out  torch.Size([100, 300])\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # first layer is fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 9 * 9),\n",
    "            nn.ReLU())\n",
    "\n",
    "        # deconvolutional filters, essentially inverse of convolutional filters\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 3, 4, stride=2),\n",
    "            #nn.Sigmoid())\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('dec')\n",
    "        #print('input ',x.size())\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.dim_h * 8, 9, 9)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "##############################################################################\n",
    "\"\"\"set models, loss functions\"\"\"\n",
    "# control which parameters are frozen / free for optimization\n",
    "def free_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def frozen_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\"\"\"functions to create SMOTE images\"\"\"\n",
    "\n",
    "def biased_get_class(c):\n",
    "    \n",
    "    xbeg = dec_x[dec_y == c]\n",
    "    ybeg = dec_y[dec_y == c]\n",
    "    \n",
    "    return xbeg, ybeg\n",
    "    #return xclass, yclass\n",
    "\n",
    "\n",
    "def G_SM(X, y,n_to_sample,cl):\n",
    "\n",
    "    # determining the number of samples to generate\n",
    "    #n_to_sample = 10 \n",
    "\n",
    "    # fitting the model\n",
    "    n_neigh = 5 + 1\n",
    "    nn = NearestNeighbors(n_neighbors=n_neigh, n_jobs=1)\n",
    "    nn.fit(X)\n",
    "    dist, ind = nn.kneighbors(X)\n",
    "\n",
    "    # generating samples\n",
    "    base_indices = np.random.choice(list(range(len(X))),n_to_sample)\n",
    "    neighbor_indices = np.random.choice(list(range(1, n_neigh)),n_to_sample)\n",
    "\n",
    "    X_base = X[base_indices]\n",
    "    X_neighbor = X[ind[base_indices, neighbor_indices]]\n",
    "\n",
    "    samples = X_base + np.multiply(np.random.rand(n_to_sample,1),\n",
    "            X_neighbor - X_base)\n",
    "\n",
    "    #use 10 as label because 0 to 9 real classes and 1 fake/smoted = 10\n",
    "    return samples, [cl]*n_to_sample\n",
    "\n",
    "#xsamp, ysamp = SM(xclass,yclass)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "#NOTE: Download the training ('.../0_trn_img.txt') and label files \n",
    "# ('.../0_trn_lab.txt').  Place the files in directories (e.g., ../MNIST/trn_img/\n",
    "# and /MNIST/trn_lab/).  Originally, when the code was written, it was for 5 fold\n",
    "#cross validation and hence there were 5 files in each of the \n",
    "#directories.  Here, for illustration, we use only 1 training and 1 label\n",
    "#file (e.g., '.../0_trn_img.txt' and '.../0_trn_lab.txt').\n",
    "\n",
    "dtrnimg = '/kaggle/input/cifar10-imb-5cv/imb_train_data'\n",
    "dtrnlab = '/kaggle/input/cifar10-imb-5cv/imb_train_label'\n",
    "\n",
    "ids = os.listdir(dtrnimg)\n",
    "idtri_f = [os.path.join(dtrnimg, image_id) for image_id in ids]\n",
    "idtri_f.sort()\n",
    "print(idtri_f)\n",
    "\n",
    "ids = os.listdir(dtrnlab)\n",
    "idtrl_f = [os.path.join(dtrnlab, image_id) for image_id in ids]\n",
    "idtrl_f.sort()\n",
    "print(idtrl_f)\n",
    "\n",
    "#for i in range(5):\n",
    "for k in range(1):\n",
    "    print()\n",
    "    print(k)\n",
    "    encoder = Encoder(args)\n",
    "    decoder = Decoder(args)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "    #decoder loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    trnimgfile = idtri_f[k]\n",
    "    trnlabfile = idtrl_f[k]\n",
    "    \n",
    "    print(trnimgfile)\n",
    "    print(trnlabfile)\n",
    "    \n",
    "    dec_x = np.loadtxt(trnimgfile)\n",
    "    dec_y = np.loadtxt(trnlabfile)\n",
    "\n",
    "    print('train imgs before reshape ',dec_x.shape) \n",
    "    print('train labels ',dec_y.shape) \n",
    "    print(collections.Counter(dec_y))\n",
    "    dec_x = dec_x.reshape(dec_x.shape[0],3,32,32)   \n",
    "    print('train imgs after reshape ',dec_x.shape) \n",
    "\n",
    "    batch_size = 100\n",
    "    num_workers = 0\n",
    "\n",
    "    #torch.Tensor returns float so if want long then use torch.tensor\n",
    "    tensor_x = torch.Tensor(dec_x)\n",
    "    tensor_y = torch.tensor(dec_y,dtype=torch.long)\n",
    "    cifar10_bal = TensorDataset(tensor_x,tensor_y) \n",
    "    train_loader = torch.utils.data.DataLoader(cifar10_bal, \n",
    "        batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "    \n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    # metric leanring##############################################################################\n",
    "    distance = distances.CosineSimilarity()\n",
    "    reducer = reducers.ThresholdReducer(low=0)\n",
    "    loss_fn = losses.TripletMarginLoss(margin=args['triplet_margin'], distance=distance, reducer=reducer)\n",
    "    miner = miners.TripletMarginMiner(margin=args['triplet_margin'], distance=distance, type_of_triplets='semihard')\n",
    "    accuracy_calculator = AccuracyCalculator(include=('precision_at_1',), k=1)\n",
    "    ###############################################################################################\n",
    "\n",
    "    t0 = time.time()\n",
    "    if args['train']:\n",
    "        enc_optim = torch.optim.Adam(encoder.parameters(), lr = args['lr'])\n",
    "        dec_optim = torch.optim.Adam(decoder.parameters(), lr = args['lr'])\n",
    "    \n",
    "        for epoch in range(args['epochs']):\n",
    "            train_loss = 0.0\n",
    "            tmse_loss = 0.0\n",
    "            tdiscr_loss = 0.0\n",
    "            contrastive_loss = 0.0\n",
    "            # train for one epoch -- set nets to train mode\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "        \n",
    "            for i, (images,labs) in enumerate(train_loader):\n",
    "            \n",
    "                # zero gradients for each batch\n",
    "                encoder.zero_grad()\n",
    "                decoder.zero_grad()\n",
    "                #print(images)\n",
    "                images, labs = images.to(device), labs.to(device)\n",
    "                #print('images ',images.size()) \n",
    "                labsn = labs.detach().cpu().numpy()\n",
    "                #print('labsn ',labsn.shape, labsn)\n",
    "            \n",
    "                # run images\n",
    "                z_hat = encoder(images)\n",
    "            \n",
    "                x_hat = decoder(z_hat) #decoder outputs tanh\n",
    "                #print('xhat ', x_hat.size())\n",
    "                #print(x_hat)\n",
    "                mse = criterion(x_hat,images)\n",
    "                #print('mse ',mse)\n",
    "                \n",
    "                       \n",
    "                resx = []\n",
    "                resy = []\n",
    "            \n",
    "                tc = np.random.choice(10,1)\n",
    "                #tc = 9\n",
    "                xbeg = dec_x[dec_y == tc]\n",
    "                ybeg = dec_y[dec_y == tc] \n",
    "                xlen = len(xbeg)\n",
    "                nsamp = min(xlen, 100)\n",
    "                ind = np.random.choice(list(range(len(xbeg))),nsamp,replace=False)\n",
    "                xclass = xbeg[ind]\n",
    "                yclass = ybeg[ind]\n",
    "            \n",
    "                xclen = len(xclass)\n",
    "                #print('xclen ',xclen)\n",
    "                xcminus = np.arange(1,xclen)\n",
    "                #print('minus ',xcminus.shape,xcminus)\n",
    "                \n",
    "                xcplus = np.append(xcminus,0)\n",
    "                #print('xcplus ',xcplus)\n",
    "                xcnew = (xclass[[xcplus],:])\n",
    "                #xcnew = np.squeeze(xcnew)\n",
    "                xcnew = xcnew.reshape(xcnew.shape[1],xcnew.shape[2],xcnew.shape[3],xcnew.shape[4])\n",
    "                #print('xcnew ',xcnew.shape)\n",
    "            \n",
    "                xcnew = torch.Tensor(xcnew)\n",
    "                xcnew = xcnew.to(device)\n",
    "            \n",
    "                #encode xclass to feature space\n",
    "                xclass = torch.Tensor(xclass)\n",
    "                xclass = xclass.to(device)\n",
    "                xclass = encoder(xclass)\n",
    "                #print('xclass ',xclass.shape) \n",
    "            \n",
    "                xclass = xclass.detach().cpu().numpy()\n",
    "            \n",
    "                xc_enc = (xclass[[xcplus],:])\n",
    "                xc_enc = np.squeeze(xc_enc)\n",
    "                #print('xc enc ',xc_enc.shape)\n",
    "            \n",
    "                xc_enc = torch.Tensor(xc_enc)\n",
    "                xc_enc = xc_enc.to(device)\n",
    "                \n",
    "                ximg = decoder(xc_enc)\n",
    "                \n",
    "                mse2 = criterion(ximg,xcnew)\n",
    "                \n",
    "                # Start contrastive loss term\n",
    "                embeddings = encoder(images)\n",
    "                indices_tuple = miner(embeddings, labs)\n",
    "                contra_loss = loss_fn(embeddings, labs, indices_tuple)\n",
    "                '''\n",
    "                if i % 100 ==0:\n",
    "                    print(\n",
    "                    \"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(\n",
    "                    epoch, i, contra_loss, miner.num_triplets)\n",
    "                    )'''\n",
    "            \n",
    "                comb_loss = mse2 + mse + contra_loss\n",
    "                comb_loss.backward()\n",
    "            \n",
    "                enc_optim.step()\n",
    "                dec_optim.step()\n",
    "            \n",
    "                train_loss += comb_loss.item()*images.size(0)\n",
    "                tmse_loss += mse.item()*images.size(0)\n",
    "                tdiscr_loss += mse2.item()*images.size(0)\n",
    "                contrastive_loss += contra_loss.item() * images.size(0)\n",
    "                 \n",
    "            # print avg training statistics \n",
    "            train_loss = train_loss/len(train_loader)\n",
    "            tmse_loss = tmse_loss/len(train_loader)\n",
    "            tdiscr_loss = tdiscr_loss/len(train_loader)\n",
    "            contrastive_loss = contrastive_loss / len(train_loader)\n",
    "            print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f} \\tcontrastive loss: {:.6f}'.format(epoch,\n",
    "                    train_loss,tmse_loss,tdiscr_loss, contrastive_loss))\n",
    "            \n",
    "        \n",
    "        \n",
    "            #store the best encoder and decoder models\n",
    "            #here, /crs5 is a reference to 5 way cross validation, but is not\n",
    "            #necessary for illustration purposes\n",
    "            if train_loss < best_loss:\n",
    "                print('Saving..')\n",
    "                #path_enc = './models/crs5/' \\\n",
    "                    #+ str(i) + '/bst_enc.pth'\n",
    "                path_enc = './'+ str(k) +'_bst_enc.pth'\n",
    "                #path_dec = './models/crs5/' \\\n",
    "                    #+ str(i) + '/bst_dec.pth'\n",
    "                path_dec = './'+ str(k) +'_bst_dec.pth'\n",
    "             \n",
    "                torch.save(encoder.state_dict(), path_enc)\n",
    "                torch.save(decoder.state_dict(), path_dec)\n",
    "        \n",
    "                best_loss = train_loss\n",
    "        \n",
    "        \n",
    "        #in addition, store the final model (may not be the best) for\n",
    "        #informational purposes\n",
    "        path_enc = './' + str(k) + '_final_enc.pth'\n",
    "        path_dec = './' + str(k) + '_final_dec.pth'\n",
    "        print(path_enc)\n",
    "        print(path_dec)\n",
    "        #torch.save(encoder.state_dict(), path_enc)\n",
    "        #torch.save(decoder.state_dict(), path_dec)\n",
    "        print()\n",
    "              \n",
    "    t1 = time.time()\n",
    "    print('total time(min): {:.2f}'.format((t1 - t0)/60))             \n",
    "\n",
    "    \n",
    "t4 = time.time()\n",
    "print('final time(min): {:.2f}'.format((t4 - t3)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7656.895259,
   "end_time": "2023-04-11T18:26:49.852927",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-11T16:19:12.957668",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
